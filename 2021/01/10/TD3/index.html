
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Twin Delayed DDPG (TD3) - Sunrisulfr的菠萝屋</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Sunrisulfr,"> 
    <meta name="description" content="虽然DDPG有时可以获得很好的性能，但对于超参数和其他类型的调优，它经常是脆弱的。DDPG最常见的失效模式是学习后的q函数开始大幅高估q值，从而导致策略失效，因为它利用了q函数中的错误。Twin D,"> 
    <meta name="author" content="Sunrisulfr"> 
    <link rel="alternative" href="atom.xml" title="Sunrisulfr的菠萝屋" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.1.1"></head>

<body class="loading">
    <span id="config-title" style="display:none">Sunrisulfr的菠萝屋</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="https://sunrisulfr.github.io"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">Twin Delayed DDPG (TD3)</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Twin Delayed DDPG (TD3)</h1>
        <div class="stuff">
            <span>一月 10, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>


        </div>
        <div class="content markdown">
            <p>虽然DDPG有时可以获得很好的性能，但对于超参数和其他类型的调优，它经常是脆弱的。DDPG最常见的失效模式是学习后的q函数开始大幅高估q值，从而导致策略失效，因为它利用了q函数中的错误。Twin Delayed DDPG (TD3)是一种通过引入三个关键技巧来解决这个问题的算法:</p>
<ul>
<li><strong>Clipped Double Q-learning</strong>: TD3学习两个q函数而不是一个(因此称为“Twin”)，并使用两个q值中较小的一个来形成Bellman误差损失函数中的目标。</li>
<li><strong>“Delayed Policy Update</strong>: TD3更新策略(和目标网络)的频率低于q函数。比如，每两个q函数更新进行一次策略更新。</li>
<li><strong>Target Policy Smoothing</strong>: TD3向目标动作添加噪声，使策略更难利用Q函数错误，方法是使Q沿着动作的变化平滑。</li>
</ul>
<h1 id="Key-Equations"><a href="#Key-Equations" class="headerlink" title="Key Equations"></a>Key Equations</h1><h2 id="Target-Policy-Smoothing"><a href="#Target-Policy-Smoothing" class="headerlink" title="Target Policy Smoothing"></a>Target Policy Smoothing</h2><p>用于形成q学习目标的动作是基于目标策略，$\mu<em>{\theta</em>{\text{targ}}} $，但是在动作的每个维度上都添加了剪切噪声。在添加了被剪辑的噪声之后，目标动作就会被剪辑到有效的动作范围内(所有有效的动作$a$，满足$a<em>{Low} \leq a \leq a</em>{High}$)。目标操作如下:</p>
<script type="math/tex; mode=display">
a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\; \epsilon \sim \mathcal{N}(0, \sigma))</script><p>目标策略平滑实质上是算法的正则化。它解决了DDPG中可能发生的特定失效模式:如果q函数近似器为某些动作开发了一个不正确的尖峰，策略将迅速利用该尖峰，然后产生脆弱或不正确的行为。这可以通过平滑类似行为的q函数来避免，这是政策平滑的目标。</p>
<h2 id="Clipped-Double-Q-learning"><a href="#Clipped-Double-Q-learning" class="headerlink" title="Clipped Double Q-learning"></a>Clipped Double Q-learning</h2><p>两个q函数都使用一个目标，使用两个q函数中的任意一个计算出一个较小的目标值:</p>
<script type="math/tex; mode=display">
y(r,s',d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s', a'(s'))</script><p>然后他们都通过回归这个目标来学习：</p>
<script type="math/tex; mode=display">
L(\phi_1, {\mathcal D}) = \mathop{E}\limits_{(s,a,r,s',d) \sim {\mathcal D}}{\Bigg( Q_{\phi_1}(s,a) - y(r,s',d) \Bigg)^2},</script><script type="math/tex; mode=display">
L(\phi_2, {\mathcal D}) = \mathop{E}\limits_{(s,a,r,s',d) \sim {\mathcal D}}{\Bigg( Q_{\phi_2}(s,a) - y(r,s',d) \Bigg)^2}.</script><p>为目标使用较小的q值，并向其回归，有助于避免q函数中的过高估计。</p>
<p>最后:通过最大化$Q_{\phi_1}$来学习策略:</p>
<script type="math/tex; mode=display">
\max_{\theta} \mathop{E}\limits_{s \sim {\mathcal D}}\left[ Q_{\phi_1}(s, \mu_{\theta}(s)) \right]</script><p>这和DDPG几乎没有什么区别。然而，在TD3中，策略的更新频率低于q函数。这有助于抑制DDPG中由于策略更新更改目标的方式而出现的波动性。</p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p><img src="/2021/01/10/TD3/TD3Algorithm.svg" alt="Algorithm"></p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_action</span>):</span></span><br><span class="line">        super(Actor, self).__init__()</span><br><span class="line">        self.f1 = nn.Linear(state_dim, <span class="number">256</span>)</span><br><span class="line">        self.f2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f3 = nn.Linear(<span class="number">128</span>, action_dim)</span><br><span class="line">        self.max_action = max_action</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f3(x)</span><br><span class="line">        <span class="keyword">return</span> torch.tanh(x) * self.max_action</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span></span><br><span class="line">        super(Critic,self).__init__()</span><br><span class="line">        self.f11 = nn.Linear(state_dim+action_dim, <span class="number">256</span>)</span><br><span class="line">        self.f12 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f13 = nn.Linear(<span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.f21 = nn.Linear(state_dim + action_dim, <span class="number">256</span>)</span><br><span class="line">        self.f22 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f23 = nn.Linear(<span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">        sa = torch.cat([state, action], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.f11(sa)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f12(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        Q1 = self.f13(x)</span><br><span class="line"></span><br><span class="line">        x = self.f21(sa)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f22(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        Q2 = self.f23(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Q1, Q2</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">self.actor = Actor(self.state_dim, self.action_dim, self.max_action)</span><br><span class="line">self.target_actor = copy.deepcopy(self.actor)</span><br><span class="line">self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.critic = Critic(self.state_dim, self.action_dim)</span><br><span class="line">self.target_critic = copy.deepcopy(self.critic)</span><br><span class="line">self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=<span class="number">3e-4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">       self.total_it += <span class="number">1</span></span><br><span class="line">       data = self.buffer.smaple(size=<span class="number">128</span>)</span><br><span class="line">       state, action, done, state_next, reward = data</span><br><span class="line">       <span class="keyword">with</span> torch.no_grad:</span><br><span class="line">           noise = (torch.rand_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)</span><br><span class="line">           next_action = (self.target_actor(state_next) + noise).clamp(-self.max_action, self.max_action)</span><br><span class="line">           target_Q1,target_Q2 = self.target_critic(state_next, next_action)</span><br><span class="line">           target_Q = torch.min(target_Q1, target_Q2)</span><br><span class="line">           target_Q = reward + done * self.discount * target_Q</span><br><span class="line">       current_Q1, current_Q2 = self.critic(state, action)</span><br><span class="line">       critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)</span><br><span class="line">       critic_loss.backward()</span><br><span class="line">       self.critic_optimizer.step()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> self.total_it % self.policy_freq == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">           q1,q2 = self.critic(state, self.actor(state))</span><br><span class="line">           actor_loss = -torch.min(q1, q2).mean()</span><br><span class="line"></span><br><span class="line">           self.actor_optimizer.zero_grad()</span><br><span class="line">           actor_loss.backward()</span><br><span class="line">           self.actor_optimizer.step()</span><br><span class="line">           <span class="keyword">for</span> param, target_param <span class="keyword">in</span> zip(self.critic.parameters(), self.target_critic.parameters()):</span><br><span class="line">               target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">for</span> param, target_param <span class="keyword">in</span> zip(self.actor.parameters(), self.target_actor.parameters()):</span><br><span class="line">               target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br></pre></td></tr></table></figure>
            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='true'
        data-ci='f16f580b288c93fceb5d'
        data-cs='b844e802d1894562ccabe326fcf989821a9a0915'
        data-r='blogissue'
        data-o='sunrisulfr'
        data-a='sunrisulfr'
        data-d='true'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
