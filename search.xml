<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>确定性策略梯度DDPG</title>
    <url>/2021/01/03/DDPG/</url>
    <content><![CDATA[<p>DDPG是google DeepMind团队提出的一种用于输出确定性动作的算法，它解决了Actor-Critic 神经网络每次参数更新前后都存在相关性，导致神经网络只能片面的看待问题这一缺点。同时也解决了DQN不能用于连续性动作的缺点。</p>
<p>论文链接：<a href="https://arxiv.org/pdf/1509.02971.pdf">https://arxiv.org/pdf/1509.02971.pdf</a></p>
<h1 id="确定性策略"><a href="#确定性策略" class="headerlink" title="确定性策略"></a>确定性策略</h1><p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成 $\pi_\theta(s) = a$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPActorCritic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, observation_space, action_space, hidden_sizes=(<span class="params"><span class="number">256</span>,<span class="number">256</span></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=nn.ReLU</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        obs_dim = observation_space.shape[<span class="number">0</span>]</span><br><span class="line">        act_dim = action_space.shape[<span class="number">0</span>]</span><br><span class="line">        act_limit = action_space.high[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build policy and value functions</span></span><br><span class="line">        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)</span><br><span class="line">        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">return</span> self.pi(obs).numpy()</span><br></pre></td></tr></table></figure>
<p>理由：</p>
<ul>
<li>即使通过PG学习得到了随机策略之后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，才能获得action的具体值；而action通常是高维的向量，比如25维、50维，在高维的action空间的频繁采样，无疑是很耗费计算能力的；</li>
<li>在PG的学习过程中，每一步计算policy gradient都需要在整个action space进行积分。</li>
</ul>
<h1 id="DPG到DDPG"><a href="#DPG到DDPG" class="headerlink" title="DPG到DDPG"></a>DPG到DDPG</h1><p>使用卷积神经网络来模拟策略函数和Q函数，并用深度学习的方法来训练，证明了在RL方法中，非线性模拟函数的准确性和高性能、可收敛；</p>
<p>而DPG中，可以看成使用线性回归的机器学习方法：使用带参数的线性函数来模拟策略函数和Q函数，然后使用线性回归的方法进行训练。</p>
<p>experience replay memory的使用：actor同环境交互时，产生的transition数据序列是在时间上高度关联(correlated)的，如果这些数据序列直接用于训练，会导致神经网络的overfit，不易收敛。</p>
<p>DDPG的actor将transition数据先存入experience replay buffer, 然后在训练时，从experience replay buffer中随机采样mini-batch数据，这样采样得到的数据可以认为是无关联的。</p>
<p>target 网络和online 网络的使用， 使的学习过程更加稳定，收敛更有保障。</p>
<h2 id="实现框架"><a href="#实现框架" class="headerlink" title="实现框架"></a>实现框架</h2><p><img src="/2021/01/03/DDPG/Framework.PNG" alt="framwork"></p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/2021/01/03/DDPG/Algorithm.PNG" alt="algorithm"></p>
<h2 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h2><p>DDPG以非策略的方式训练确定性策略。因为策略是确定的，如果agent要探索on-policy，在一开始它可能不会尝试足够广泛的行动来找到有用的学习信号。为了使DDPG政策更好地探索，我们在训练时为他们的行动添加噪音。原始DDPG论文的作者推荐了时间相关的OU噪声，但最近的结果表明，不相关、均值为零的高斯噪声工作得很好。因为后者更简单，所以更可取。为了便于获得高质量的训练数据，您可以在训练过程中减少噪声的规模。(我们在实现中没有这样做，并始终保持噪声规模固定。)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_action</span>(<span class="params">o, noise_scale</span>):</span></span><br><span class="line">        a = ac.act(torch.as_tensor(o, dtype=torch.float32))</span><br><span class="line">        a += noise_scale * np.random.randn(act_dim)</span><br><span class="line">        <span class="keyword">return</span> np.clip(a, -act_limit, act_limit)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Imitation Learning (GAIL)</title>
    <url>/2022/03/03/GAIL/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>论文：<a href="https://arxiv.org/abs/1606.03476">https://arxiv.org/abs/1606.03476</a><br>本文要解决的是Imitation Learning和Inverse RL。基本思想比较简单，利用GAN的对抗训练来生成给定的专家数据分布。为什么要用GAN，作者提出，一般Imitation Learning传统的Behavioral Cloning的方法存在状态漂移的问题，一旦遇到没有在专家轨迹中出现的状态将会产生很大的误差以及累计误差；此外，Inverse RL逆强化学习的方法把强化学习的学习过程套在求解cost function的过程中，因此效率很低；然后，逆强化学习只学到的cost function只是解释了专家轨迹，但没有学习到策略，而利用GAIL可以直接显式的得到决策，更高效。</p>
<p>GAIL的核心在于，尽管使用了对抗的思想，但并没有显式的Generator在其中，充当Generator作用的是智能体的Policy。GAIL的学习大致分为两步，第一步通过当前policy采样得到的数据与专家数据进行对抗训练来训练Discriminator；然后，利用Discriminator作为surrogate reward function来训练策略Policy，文章使用的TRPO。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 日记：剑指offer篇</title>
    <url>/2022/03/01/Leetcode/</url>
    <content><![CDATA[<p>我真的不想刷题！！ T_T</p>
<h1 id="剑指-Offer-33-二叉搜索树的后序遍历序列"><a href="#剑指-Offer-33-二叉搜索树的后序遍历序列" class="headerlink" title="剑指 Offer 33. 二叉搜索树的后序遍历序列"></a>剑指 Offer 33. 二叉搜索树的后序遍历序列</h1><h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><p>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。</p>
<p>参考以下这颗二叉搜索树：</p>
<pre><code>    5
   / \
  2   6
 / \
1   3
</code></pre><p>示例 1：</p>
<p>输入: [1,6,3,2,5]<br>输出: false</p>
<p>示例 2：</p>
<p>输入: [1,3,2,6,5]<br>输出: true</p>
<h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// 单调栈</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">verifyPostorder</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; postorder)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">        <span class="keyword">int</span> root = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = postorder.size() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">            <span class="keyword">if</span>(postorder[i] &gt; root)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span>(!s.empty() &amp;&amp; postorder[i] &lt; s.top())&#123;</span><br><span class="line">                root = s.top();</span><br><span class="line">                s.pop();</span><br><span class="line">            &#125;</span><br><span class="line">            s.push(postorder[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="剑指-Offer-36-二叉搜索树与双向链表"><a href="#剑指-Offer-36-二叉搜索树与双向链表" class="headerlink" title="剑指 Offer 36. 二叉搜索树与双向链表"></a>剑指 Offer 36. 二叉搜索树与双向链表</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的循环双向链表。要求不能创建任何新的节点，只能调整树中节点指针的指向。</p>
<p>code<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Node *pre = <span class="literal">nullptr</span>, *head = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(Node* root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span>;</span><br><span class="line">        dfs(root -&gt; left);</span><br><span class="line">        <span class="keyword">if</span> (pre) pre-&gt;right = root;</span><br><span class="line">        <span class="keyword">else</span> head = root; <span class="comment">// 保存链表头结点</span></span><br><span class="line">        root-&gt;left = pre; </span><br><span class="line">        pre = root;</span><br><span class="line">        dfs(root-&gt;right); <span class="comment">//右子树</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Node* <span class="title">treeToDoublyList</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> root;</span><br><span class="line">        dfs(root);</span><br><span class="line">        head-&gt;left = pre;</span><br><span class="line">        pre-&gt;right = head;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h1 id="剑指-Offer-II-078-合并排序链表"><a href="#剑指-Offer-II-078-合并排序链表" class="headerlink" title="剑指 Offer II 078. 合并排序链表"></a>剑指 Offer II 078. 合并排序链表</h1><h2 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个链表数组，每个链表都已经按升序排列。<br>请将所有链表合并到一个升序链表中，返回合并后的链表。</p>
 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">cmp</span>&#123;</span></span><br><span class="line">        <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(ListNode* a, ListNode* b)</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> a -&gt; val &gt; b -&gt; val;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(<span class="built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">priority_queue</span>&lt;ListNode *, <span class="built_in">vector</span>&lt;ListNode *&gt;, cmp&gt; pq;</span><br><span class="line">        ListNode* dummy = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> p : lists)&#123;</span><br><span class="line">            <span class="keyword">if</span>(p) pq.push(p);</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode* t = dummy;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!pq.empty())&#123;</span><br><span class="line">            t -&gt; next = pq.top();</span><br><span class="line">            t = pq.top();</span><br><span class="line">            pq.pop();</span><br><span class="line">            <span class="keyword">if</span>(t-&gt;next)&#123;</span><br><span class="line">                pq.push(t-&gt;next);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dummy-&gt;next;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="剑指-Offer-II-106-二分图"><a href="#剑指-Offer-II-106-二分图" class="headerlink" title="剑指 Offer II 106. 二分图"></a>剑指 Offer II 106. 二分图</h1> <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isBipartite</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; graph)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = graph.size();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">color</span><span class="params">(n, <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(!color[i])&#123;</span><br><span class="line">                q.push(i);</span><br><span class="line">                color[i] = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span>(!q.empty())&#123;</span><br><span class="line">                <span class="keyword">int</span> node = q.front();</span><br><span class="line">                q.pop();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">int</span> &amp; j : graph[node])&#123;</span><br><span class="line">                    <span class="keyword">if</span>(color[j] == <span class="number">0</span>)&#123;</span><br><span class="line">                        color[j] = color[node] == <span class="number">2</span> ? <span class="number">1</span> : <span class="number">2</span>;</span><br><span class="line">                        q.push(j);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(color[node] == color[j])</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Soft Actor Critic (SAC)</title>
    <url>/2021/01/20/SAC/</url>
    <content><![CDATA[<p>SAC (Soft Actor Critic)是一种对随机策略进行非策略优化的算法，在随机策略优化和ddpg方法之间架起了一座桥梁。它并不是TD3的直接继承者(几乎是同时发布的)，但它包含了短双q技巧，并且由于SAC政策的固有随机性，它也最终受益于目标政策平滑之类的东西。</p>
<p>SAC的一个中心特征是熵正则化。该策略被训练成最大化期望收益和熵之间的权衡，熵是该策略中随机性的度量。这与探索-利用的权衡关系密切:熵的增加会导致更多的探索，从而加速以后的学习。它还可以防止策略过早地收敛到一个坏的局部最优。</p>
<h1 id="Entropy-Regularized-Reinforcement-Learning"><a href="#Entropy-Regularized-Reinforcement-Learning" class="headerlink" title="Entropy-Regularized Reinforcement Learning"></a>Entropy-Regularized Reinforcement Learning</h1><p>Entropy is a quantity which, roughly speaking, says how random a random variable is. If a coin is weighted so that it almost always comes up heads, it has low entropy; if it’s evenly weighted and has a half chance of either outcome, it has high entropy.</p>
<p>Let x be a random variable with probability mass or density function P. The entropy H of x is computed from its distribution P according to</p>
<script type="math/tex; mode=display">
H(P) = \mathop{\text{E}}\limits_{x\sim P}[-\log P(x)].</script><p>在熵正则化强化学习中，agent在每个时间步上获得的奖励与策略在该时间步上的熵成正比。这将RL问题改变为:</p>
<script type="math/tex; mode=display">
\pi^* = \arg \max_{\pi} \mathop{\text{E}}\limits_{\tau \sim \pi}{ \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg)},</script><p>其中$\alpha &gt; 0$是权衡系数。(注意:我们在这里假设一个无限视界贴现设置，我们将在本页面的其余部分做同样的事情。)现在我们可以在这个设置中定义稍微不同的值函数。$V^{\pi}$被修改为包含每个时间步的熵加成:</p>
<script type="math/tex; mode=display">V^{\pi}(s) = \mathop{\text{E}}\limits_{\tau \sim \pi} \Bigg [ \left. \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg) \right| s_0 = s \Bigg ]</script><p>$Q^{\pi}$被修改为包含除了第一个时间步之外的每个时间步的熵奖励:</p>
<script type="math/tex; mode=display">Q^{\pi}(s,a) = \mathop{\text{E}}\limits_{\tau \sim \pi}{ \Bigg [\left. \sum_{t=0}^{\infty} \gamma^t  R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t H\left(\pi(\cdot|s_t)\right)\right| s_0 = s, a_0 = a}\Bigg ]</script><p>有了这些定义，$V^{\pi}$和$Q^{\pi}$之间是这样连接的:</p>
<script type="math/tex; mode=display">
V^\pi(s) = \mathop{\text{E}}\limits_{a \sim \pi}[Q^\pi(s,a)] + \alpha H(\pi(\cdot | s))</script><p>$Q^{\pi}$的Bellman方程为:</p>
<script type="math/tex; mode=display">
Q^{\pi}(s,a) = \mathop{\text{E}}\limits_{s' \sim P,  a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
= \mathop{\text{E}}\limits_{s' \sim P}{R(s,a,s') + \gamma V^{\pi}(s')}.</script><h1 id="Soft-Actor-critic"><a href="#Soft-Actor-critic" class="headerlink" title="Soft Actor-critic"></a>Soft Actor-critic</h1><p>SAC同时学习一个策略$pi<em>{\ θ}$和两个$q$函数$Q</em>{\phi<em>1}$，$Q</em>{\phi_2}$。目前标准的SAC有两种变体:一种使用固定的熵正则化系数$\alpha$，另一种通过在培训过程中改变$\alpha$来强制熵约束。</p>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><p><img src="/2021/01/20/SAC/Algorithm.svg" alt="algorithm"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>off-policy</tag>
      </tags>
  </entry>
  <entry>
    <title>Twin Delayed DDPG (TD3)</title>
    <url>/2021/01/10/TD3/</url>
    <content><![CDATA[<p>虽然DDPG有时可以获得很好的性能，但对于超参数和其他类型的调优，它经常是脆弱的。DDPG最常见的失效模式是学习后的q函数开始大幅高估q值，从而导致策略失效，因为它利用了q函数中的错误。Twin Delayed DDPG (TD3)是一种通过引入三个关键技巧来解决这个问题的算法:</p>
<ul>
<li><strong>Clipped Double Q-learning</strong>: TD3学习两个q函数而不是一个(因此称为“Twin”)，并使用两个q值中较小的一个来形成Bellman误差损失函数中的目标。</li>
<li><strong>“Delayed Policy Update</strong>: TD3更新策略(和目标网络)的频率低于q函数。比如，每两个q函数更新进行一次策略更新。</li>
<li><strong>Target Policy Smoothing</strong>: TD3向目标动作添加噪声，使策略更难利用Q函数错误，方法是使Q沿着动作的变化平滑。</li>
</ul>
<h1 id="Key-Equations"><a href="#Key-Equations" class="headerlink" title="Key Equations"></a>Key Equations</h1><h2 id="Target-Policy-Smoothing"><a href="#Target-Policy-Smoothing" class="headerlink" title="Target Policy Smoothing"></a>Target Policy Smoothing</h2><p>用于形成q学习目标的动作是基于目标策略，$\mu<em>{\theta</em>{\text{targ}}} $，但是在动作的每个维度上都添加了剪切噪声。在添加了被剪辑的噪声之后，目标动作就会被剪辑到有效的动作范围内(所有有效的动作$a$，满足$a<em>{Low} \leq a \leq a</em>{High}$)。目标操作如下:</p>
<script type="math/tex; mode=display">
a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\; \epsilon \sim \mathcal{N}(0, \sigma))</script><p>目标策略平滑实质上是算法的正则化。它解决了DDPG中可能发生的特定失效模式:如果q函数近似器为某些动作开发了一个不正确的尖峰，策略将迅速利用该尖峰，然后产生脆弱或不正确的行为。这可以通过平滑类似行为的q函数来避免，这是政策平滑的目标。</p>
<h2 id="Clipped-Double-Q-learning"><a href="#Clipped-Double-Q-learning" class="headerlink" title="Clipped Double Q-learning"></a>Clipped Double Q-learning</h2><p>两个q函数都使用一个目标，使用两个q函数中的任意一个计算出一个较小的目标值:</p>
<script type="math/tex; mode=display">
y(r,s',d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s', a'(s'))</script><p>然后他们都通过回归这个目标来学习：</p>
<script type="math/tex; mode=display">
L(\phi_1, {\mathcal D}) = \mathop{E}\limits_{(s,a,r,s',d) \sim {\mathcal D}}{\Bigg( Q_{\phi_1}(s,a) - y(r,s',d) \Bigg)^2},</script><script type="math/tex; mode=display">
L(\phi_2, {\mathcal D}) = \mathop{E}\limits_{(s,a,r,s',d) \sim {\mathcal D}}{\Bigg( Q_{\phi_2}(s,a) - y(r,s',d) \Bigg)^2}.</script><p>为目标使用较小的q值，并向其回归，有助于避免q函数中的过高估计。</p>
<p>最后:通过最大化$Q_{\phi_1}$来学习策略:</p>
<script type="math/tex; mode=display">
\max_{\theta} \mathop{E}\limits_{s \sim {\mathcal D}}\left[ Q_{\phi_1}(s, \mu_{\theta}(s)) \right]</script><p>这和DDPG几乎没有什么区别。然而，在TD3中，策略的更新频率低于q函数。这有助于抑制DDPG中由于策略更新更改目标的方式而出现的波动性。</p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p><img src="/2021/01/10/TD3/TD3Algorithm.svg" alt="Algorithm"></p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_action</span>):</span></span><br><span class="line">        super(Actor, self).__init__()</span><br><span class="line">        self.f1 = nn.Linear(state_dim, <span class="number">256</span>)</span><br><span class="line">        self.f2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f3 = nn.Linear(<span class="number">128</span>, action_dim)</span><br><span class="line">        self.max_action = max_action</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f3(x)</span><br><span class="line">        <span class="keyword">return</span> torch.tanh(x) * self.max_action</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span></span><br><span class="line">        super(Critic,self).__init__()</span><br><span class="line">        self.f11 = nn.Linear(state_dim+action_dim, <span class="number">256</span>)</span><br><span class="line">        self.f12 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f13 = nn.Linear(<span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.f21 = nn.Linear(state_dim + action_dim, <span class="number">256</span>)</span><br><span class="line">        self.f22 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.f23 = nn.Linear(<span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">        sa = torch.cat([state, action], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.f11(sa)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f12(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        Q1 = self.f13(x)</span><br><span class="line"></span><br><span class="line">        x = self.f21(sa)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.f22(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        Q2 = self.f23(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Q1, Q2</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">self.actor = Actor(self.state_dim, self.action_dim, self.max_action)</span><br><span class="line">self.target_actor = copy.deepcopy(self.actor)</span><br><span class="line">self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.critic = Critic(self.state_dim, self.action_dim)</span><br><span class="line">self.target_critic = copy.deepcopy(self.critic)</span><br><span class="line">self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=<span class="number">3e-4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">       self.total_it += <span class="number">1</span></span><br><span class="line">       data = self.buffer.smaple(size=<span class="number">128</span>)</span><br><span class="line">       state, action, done, state_next, reward = data</span><br><span class="line">       <span class="keyword">with</span> torch.no_grad:</span><br><span class="line">           noise = (torch.rand_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)</span><br><span class="line">           next_action = (self.target_actor(state_next) + noise).clamp(-self.max_action, self.max_action)</span><br><span class="line">           target_Q1,target_Q2 = self.target_critic(state_next, next_action)</span><br><span class="line">           target_Q = torch.min(target_Q1, target_Q2)</span><br><span class="line">           target_Q = reward + done * self.discount * target_Q</span><br><span class="line">       current_Q1, current_Q2 = self.critic(state, action)</span><br><span class="line">       critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)</span><br><span class="line">       critic_loss.backward()</span><br><span class="line">       self.critic_optimizer.step()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> self.total_it % self.policy_freq == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">           q1,q2 = self.critic(state, self.actor(state))</span><br><span class="line">           actor_loss = -torch.min(q1, q2).mean()</span><br><span class="line"></span><br><span class="line">           self.actor_optimizer.zero_grad()</span><br><span class="line">           actor_loss.backward()</span><br><span class="line">           self.actor_optimizer.step()</span><br><span class="line">           <span class="keyword">for</span> param, target_param <span class="keyword">in</span> zip(self.critic.parameters(), self.target_critic.parameters()):</span><br><span class="line">               target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">for</span> param, target_param <span class="keyword">in</span> zip(self.actor.parameters(), self.target_actor.parameters()):</span><br><span class="line">               target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>AlphaGO：详解</title>
    <url>/2020/12/31/alphago/</url>
    <content><![CDATA[<h1 id="了解AlphaGO"><a href="#了解AlphaGO" class="headerlink" title="了解AlphaGO"></a>了解AlphaGO</h1><!-- ![Ray_head_logo](AlphaGO/AlphaGo.jpg) -->
<h2 id="选出好棋-增强学习Reinforcement-Learning"><a href="#选出好棋-增强学习Reinforcement-Learning" class="headerlink" title="选出好棋-增强学习Reinforcement Learning"></a>选出好棋-增强学习Reinforcement Learning</h2><p><img src="/2020/12/31/alphago/Network.PNG" alt="Network"></p>
<h3 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h3><h3 id="增强学习"><a href="#增强学习" class="headerlink" title="增强学习"></a>增强学习</h3><p><img src="/2020/12/31/alphago/RL.PNG" alt="RL"></p>
<h2 id="“手下一步棋，心想三步棋”-蒙特卡罗树MCTS"><a href="#“手下一步棋，心想三步棋”-蒙特卡罗树MCTS" class="headerlink" title="“手下一步棋，心想三步棋”-蒙特卡罗树MCTS"></a>“手下一步棋，心想三步棋”-蒙特卡罗树MCTS</h2><p>围棋问题实际上是一个树的搜索问题，当前局面是树的根，树根有多少分支，对应着下一步有多少对应的落子，这是树的宽度。之后树不断的生长（推演，模拟），直到叶子节点（开始落子）。从树根到叶子节点，分了多少次枝就是树的深度。树的广度越宽，深度越深，搜索所需要的时间越长。如：围棋一共361个交叉点，越往后，可以落子的位子越少，所以平均下来树的宽度大约为250，深度大约150.如果想遍历整个围棋树，需要搜索250的150次方。所以走一步前，需要搜索折磨多次数是不切实际的。</p>
<p>每次AlphaGo都会自己和自己下棋，每一步都由一个函数决定应该走哪一步，它会考虑如下几个点：</p>
<p>1.这个局面大概该怎么下？（使用SL Policy network）</p>
<p>2.下一步会导致什么样的局面？</p>
<p>3.我赢的概率是多少？（使用Value network+rollout）</p>
<p>首先，“走棋网络”是训练的当前局面s的下一步走棋位置的概率分布，它模拟的是在某个局面下，人类的常见的走棋行为，并不评估走棋之后是否赢棋（区别概率分布与赢棋的概率？）。所以，我们可以假设优秀的走棋方法是在人类常见的走棋范围内的，这样就大大减少了搜索树的宽度。</p>
<p>这时候，使用SL policy network 完成了第一落子，假设走了a1之后，然后对方开始走a2，接着我在走a3.这样一步步的模拟下去…..(这里使用两个SL policy network的自我对弈)假设V(s,a1)赢棋的概率为70%,对方走了V(s,a1,a2)对方赢棋的概率为60%。而走到第三步的时候，我方的赢棋概率V(s,a1,a2,a3)是35%，这时候还要不要在走a1呢？</p>
<p>重新定义V(s)的实际意义：它用来预测该局面以监督学习的策略网络（SL Policy network）自我对弈后赢棋的概率,也就是模拟N次后，AlphaGo认为她走这步棋赢的概率，这个概率是不断的更新。我们用V<em>表示某一局面赢棋的概率。刚开始v</em>(s,a1)=70%,在下完第三步后更新为v*(s,a1)=(70%-60%+35%)/3=15%，这个时候V(s,a1)=15%，已经不是之前的70%了，也就是说这个位置可能不是赢棋的概率最大的位置，所以舍弃。</p>
<p>然而发现，SL Policy network 来进行自我对弈太慢了(3毫秒)，重新训练了一个mini的SL Policy network，叫做rollout(2微秒). 它的输入比policy network 小，它的模型也小，它没有Policy network 准，但是比他快。</p>
<p>这就是蒙特卡罗树搜索的基本过程，</p>
<p>1.它首先使用SL policy network选出可能的落子区域，中间开始使用value network来选择可能的落子区域。</p>
<p>2.使用快速走子（Fast rollout）来适当牺牲走棋质量的条件下，通过Fast rollout的自我对弈（快速模拟）出最大概率的落子方案，使得AlphaGo 能够看到未来。</p>
<p>3.使用Value network对当前形势做一个判断，判断赢的概率，使得AlphaGo能够看到当下。可见，MC树融合了policy network 、Fast rollout和Value network，使之形成一个完整的系统。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>MCTS</tag>
      </tags>
  </entry>
  <entry>
    <title>Ray.Tune初探（一）：分布式超参优化</title>
    <url>/2020/11/03/ray1/</url>
    <content><![CDATA[<h1 id="OVERVIEW-RAY"><a href="#OVERVIEW-RAY" class="headerlink" title="OVERVIEW RAY"></a>OVERVIEW RAY</h1><p><img src="/2020/11/03/ray1/ray_header_logo.png" alt="Ray_head_logo"></p>
<h2 id="What-is-Ray"><a href="#What-is-Ray" class="headerlink" title="What is Ray?"></a>What is Ray?</h2><p><strong>Ray 提供了一个简单的通用 API，用于构建分布式应用程序。</strong></p>
<p>Ray通过以下三步完成此任务：</p>
<ol>
<li>为构建和运行分布式应用程序提供简单的基元。</li>
<li>使最终用户能够并行化单个计算机代码，很少或零更改代码。</li>
<li>在核心 Ray 上包括一个大型应用程序、库和工具生态系统，以启用复杂的应用程序。</li>
</ol>
<p><strong>Ray Core</strong>为应用程序构建提供了简单的基元。</p>
<p>在<strong>Ray Core</strong>的顶部是解决机器学习中问题的几个库：</p>
<ul>
<li><a href="https://docs.ray.io/en/latest/tune/index.html">Tune: Scalable Hyperparameter Tuning</a></li>
<li><a href="https://docs.ray.io/en/latest/rllib.html#rllib-index">RLlib: Scalable Reinforcement Learning</a></li>
<li><a href="https://docs.ray.io/en/latest/raysgd/raysgd.html#sgd-index">RaySGD: Distributed Training Wrappers</a></li>
<li><a href="https://docs.ray.io/en/latest/serve/index.html#rayserve">Ray Serve: Scalable and Programmable Serving</a></li>
</ul>
<h2 id="A-Simple-example"><a href="#A-Simple-example" class="headerlink" title="A Simple example"></a>A Simple example</h2><p>Ray 提供 Python 和 Java API。Ray 使用Tasks（functions）和Actors（Classes）两种形式来允许用户并行化代码。这里给出一个官方文档的Python示例，具体可以见<a href="https://docs.ray.io/en/latest/ray-overview/index.html#gentle-intro">A Gentle Introduction to Ray</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># First, run `pip install ray`.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ray</span><br><span class="line">ray.init()</span><br><span class="line"></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line">futures = [f.remote(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line">print(ray.get(futures)) <span class="comment"># [0, 1, 4, 9]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Counter</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">increment</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.n</span><br><span class="line"></span><br><span class="line">counters = [Counter.remote() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line">[c.increment.remote() <span class="keyword">for</span> c <span class="keyword">in</span> counters]</span><br><span class="line">futures = [c.read.remote() <span class="keyword">for</span> c <span class="keyword">in</span> counters]</span><br><span class="line">print(ray.get(futures)) <span class="comment"># [1, 1, 1, 1]</span></span><br></pre></td></tr></table></figure>
<h1 id="Waiting-for-Partial-Results"><a href="#Waiting-for-Partial-Results" class="headerlink" title="Waiting for Partial Results"></a>Waiting for Partial Results</h1><p>After launching a number of tasks, you may want to know which ones have finished executing without blocking on all of them, as in ray.get. This can be done with wait (ray.wait). The function works as follows.</p>
<h1 id="Tune-Scalable-Hyperparameter-Tuning"><a href="#Tune-Scalable-Hyperparameter-Tuning" class="headerlink" title="Tune: Scalable Hyperparameter Tuning"></a>Tune: Scalable Hyperparameter Tuning</h1><p><img src="/2020/11/03/ray1/tune_logo.png" alt="tune_logo"></p>
<h2 id="What-is-Ray-tune"><a href="#What-is-Ray-tune" class="headerlink" title="What is Ray.tune?"></a>What is Ray.tune?</h2><p>Tune is a Python library for experiment execution and hyperparameter tuning at any scale. Core features:</p>
<ul>
<li>Launch a multi-node <a href="https://docs.ray.io/en/latest/tune/tutorials/tune-distributed.html#tune-distributed">distributed hyperparameter sweep</a> in less than 10 lines of code.</li>
<li>Supports any machine learning framework, <a href="https://docs.ray.io/en/latest/tune/tutorials/overview.html#tune-guides">including PyTorch, XGBoost, MXNet, and Keras</a>.</li>
<li>Automatically manages <a href="https://docs.ray.io/en/latest/tune/user-guide.html#tune-checkpoint">checkpoints</a> and logging to <a href="https://docs.ray.io/en/latest/tune/user-guide.html#tune-logging">TensorBoard</a>.</li>
<li>Choose among state of the art algorithms such as <a href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-pbt">Population Based Training (PBT)</a>, <a href="https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#bayesopt">BayesOptSearch</a>, <a href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-hyperband">HyperBand/ASHA</a>.</li>
<li>Move your models from training to serving on the same infrastructure with <a href="https://docs.ray.io/en/latest/tune/serve/index.html">Ray Serve</a>.</li>
</ul>
<h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p>首先，需要简单地理解一下Ray.tune中的几个重要概念：</p>
<ul>
<li>Trial：Trial 是一次尝试，它会使用某组配置（例如，一组超参值，或者特定的神经网络架构）来进行训练，并返回该配置下的得分。本质上就是加入了NNIAPI的用户的原始代码。</li>
<li>Experiment：实验是一次寻找模型的最佳超参组合，或最好的神经网络架构的整个过程。 它由Trial和AutoML算法所组成。</li>
<li>Searchspace：搜索空间是模型调优的范围。 例如，超参的取值范围。</li>
<li>Configuration：配置是来自搜索空间的一个参数实例，每个超参都会有一个特定的值。</li>
<li>Tuner: Tuner就是具体的AutoML算法，会为下一个Trial生成新的配置。新的 Trial 会使用这组配置来运行。</li>
<li>Assessor：Assessor分析Trial的中间结果（例如，测试数据集上定期的精度），来确定 Trial 是否应该被提前终止。</li>
<li>Training Platform：训练平台是Trial的执行环境。根据Experiment的配置，可以是本机，远程服务器组，或其它大规模训练平台（例如，Ray自行提供的平台）</li>
</ul>
<p>那么你的实验（Experiment）便是在一定的搜索空间（Searchspace）内寻找最优的一组超参数组合（Configuration），使得该组参数对应的Trail会获得最高的准确率或者得分，在有限的时间和资源限制下，Tuner和Assessor会自动地帮助你更快更好的找到这组参数。</p>
<h2 id="快速入手实例"><a href="#快速入手实例" class="headerlink" title="快速入手实例"></a>快速入手实例</h2><p>为了快速上手，理解Ray.tune的工作流程，不妨训练一个简单的Mnist手写数字识别，网络结构确定后，Ray.tune可以来帮你找到最优的超参。</p>
<h3 id="Pytorch-Model-Setup"><a href="#Pytorch-Model-Setup" class="headerlink" title="Pytorch Model Setup"></a>Pytorch Model Setup</h3><p>首先需要搭建一个能够正常训练的模型，这一部分与Tune本身是无关的。</p>
<p>导入一些依赖项：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> tune</span><br><span class="line"><span class="keyword">from</span> ray.tune.schedulers <span class="keyword">import</span> ASHAScheduler</span><br></pre></td></tr></table></figure>
<p>然后使用PyTorch定义一个简单的神经网络模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line">        <span class="comment"># In this example, we don&#x27;t change the model architecture</span></span><br><span class="line">        <span class="comment"># due to simplicity.</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">3</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">192</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">3</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">192</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>然后定义具体的训练函数和测试函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Change these values if you want the training to run quicker or slower.</span></span><br><span class="line">EPOCH_SIZE = <span class="number">512</span></span><br><span class="line">TEST_SIZE = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, optimizer, train_loader</span>):</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment"># We set this just for the example to run quickly.</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx * len(data) &gt; EPOCH_SIZE:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">model, data_loader</span>):</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model.eval()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">            <span class="comment"># We set this just for the example to run quickly.</span></span><br><span class="line">            <span class="keyword">if</span> batch_idx * len(data) &gt; TEST_SIZE:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            outputs = model(data)</span><br><span class="line">            _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == target).sum().item()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br></pre></td></tr></table></figure>
<h3 id="Setting-up-Tune"><a href="#Setting-up-Tune" class="headerlink" title="Setting up Tune"></a>Setting up Tune</h3><p>现在，需要定义一个可以用于并行训练模型的函数。这个函数将在每一个 <a href="https://docs.ray.io/en/latest/actors.html#actor-guide">Ray Actor (process)</a> 上单独执行。因此，该函数需要将模型的性能传达回Tune主进程中。为此，在训练函数中调用 tune.report，它能够将性能的数值发送回Tune。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_mnist</span>(<span class="params">config</span>):</span></span><br><span class="line">    <span class="comment"># Data Setup</span></span><br><span class="line">    mnist_transforms = transforms.Compose(</span><br><span class="line">        [transforms.ToTensor(),</span><br><span class="line">         transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))])</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&quot;~/data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=mnist_transforms),</span><br><span class="line">        batch_size=<span class="number">64</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_loader = DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&quot;~/data&quot;</span>, train=<span class="literal">False</span>, transform=mnist_transforms),</span><br><span class="line">        batch_size=<span class="number">64</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model = ConvNet()</span><br><span class="line">    optimizer = optim.SGD(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">&quot;lr&quot;</span>], momentum=config[<span class="string">&quot;momentum&quot;</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        train(model, optimizer, train_loader)</span><br><span class="line">        acc = test(model, test_loader)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Send the current training result back to Tune</span></span><br><span class="line">        tune.report(mean_accuracy=acc)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># This saves the model to the trial directory</span></span><br><span class="line">            torch.save(model.state_dict(), <span class="string">&quot;./model.pth&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>为超参建立搜索空间后，调用 tune.run 就可以完成一次Experiment了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">search_space = &#123;</span><br><span class="line">    <span class="string">&quot;lr&quot;</span>: tune.sample_from(<span class="keyword">lambda</span> spec: <span class="number">10</span>**(<span class="number">-10</span> * np.random.rand())),</span><br><span class="line">    <span class="string">&quot;momentum&quot;</span>: tune.uniform(<span class="number">0.1</span>, <span class="number">0.9</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment this to enable distributed execution</span></span><br><span class="line"><span class="comment"># `ray.init(address=&quot;auto&quot;)`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Download the dataset first</span></span><br><span class="line">datasets.MNIST(<span class="string">&quot;~/data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">analysis = tune.run(train_mnist, config=search_space)</span><br></pre></td></tr></table></figure>
<p>tune.run 会返回一个Analysis Object。它保存了优化过程的数据，可以绘图进行分析。同时每一个trial的数据自动保存在 ~/ray_results 文件夹中，也可以使用Tesnorboard来分析优化过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir ~/ray_results</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/03/ray1/ray1_tensorboard.png" alt="tensorboard"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Ray</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Ray.Tune初探（二）：Guide</title>
    <url>/2020/12/03/ray2/</url>
    <content><![CDATA[<h1 id="Tune：使用指南"><a href="#Tune：使用指南" class="headerlink" title="Tune：使用指南"></a>Tune：使用指南</h1><h2 id="资源-Parallelism-GPUs-Distributed"><a href="#资源-Parallelism-GPUs-Distributed" class="headerlink" title="资源 (Parallelism, GPUs, Distributed)"></a>资源 (Parallelism, GPUs, Distributed)</h2><p>Parallelism is determined by <code>resources_per_trial</code> (defaulting to 1 CPU, 0 GPU per trial) and the resources available to Tune (<code>ray.cluster_resources()</code>).</p>
<p>By default, Tune automatically runs N concurrent trials, where N is the number of CPUs (cores) on your machine.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># If you have 4 CPUs on your machine, this will run 4 concurrent trials at a time.</span></span><br><span class="line">tune.run(trainable, num_samples=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>You can override this parallelism with <code>resources_per_trial</code>. Here you can specify your resource requests using either a dictionary or a PlacementGroupFactory object. In any case, Ray Tune will try to start a placement group for each trial.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># If you have 4 CPUs on your machine, this will run 2 concurrent trials at a time.</span></span><br><span class="line">tune.run(trainable, num_samples=<span class="number">10</span>, resources_per_trial=&#123;<span class="string">&quot;cpu&quot;</span>: <span class="number">2</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you have 4 CPUs on your machine, this will run 1 trial at a time.</span></span><br><span class="line">tune.run(trainable, num_samples=<span class="number">10</span>, resources_per_trial=&#123;<span class="string">&quot;cpu&quot;</span>: <span class="number">4</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fractional values are also supported, (i.e., &#123;&quot;cpu&quot;: 0.5&#125;).</span></span><br><span class="line">tune.run(trainable, num_samples=<span class="number">10</span>, resources_per_trial=&#123;<span class="string">&quot;cpu&quot;</span>: <span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="搜索空间-Grid-Random"><a href="#搜索空间-Grid-Random" class="headerlink" title="搜索空间 (Grid/Random)"></a>搜索空间 (Grid/Random)</h2><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Ray</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础</title>
    <url>/2022/03/02/ML1/</url>
    <content><![CDATA[<ul>
<li><a href="#偏差与方差">偏差与方差</a><ul>
<li><a href="#导致偏差和方差的原因">导致偏差和方差的原因</a></li>
<li><a href="#深度学习中的偏差与方差">深度学习中的偏差与方差</a></li>
<li><a href="#偏差方差-与-boostingbagging">偏差/方差 与 Boosting/Bagging</a></li>
<li><a href="#偏差与方差的计算公式">偏差与方差的计算公式</a></li>
<li><a href="#偏差与方差的权衡过拟合与模型复杂度的权衡">偏差与方差的权衡（过拟合与模型复杂度的权衡）</a></li>
</ul>
</li>
<li><a href="#生成模型与判别模型">生成模型与判别模型</a></li>
<li><a href="#先验概率与后验概率">先验概率与后验概率</a></li>
</ul>
<!-- /TOC -->
<h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><blockquote>
<p>《机器学习》 2.5 偏差与方差 - 周志华</p>
<ul>
<li><strong>偏差</strong>与<strong>方差</strong>分别是用于衡量一个模型<strong>泛化误差</strong>的两个方面；<ul>
<li>模型的<strong>偏差</strong>，指的是模型预测的<strong>期望值</strong>与<strong>真实值</strong>之间的差；</li>
<li>模型的<strong>方差</strong>，指的是模型预测的<strong>期望值</strong>与<strong>预测值</strong>之间的差平方和；</li>
</ul>
</li>
<li>在<strong>监督学习</strong>中，模型的<strong>泛化误差</strong>可<strong>分解</strong>为偏差、方差与噪声之和。<div align="center"><img src="/2022/03/02/ML1/TIM截图20180817204652.png" height></div>

</li>
</ul>
</blockquote>
<ul>
<li><strong>偏差</strong>用于描述模型的<strong>拟合能力</strong>；<br><br><strong>方差</strong>用于描述模型的<strong>稳定性</strong>。<div align="center"><img src="/2022/03/02/ML1/TIM截图20180817192259.png" height></div>

</li>
</ul>
<h3 id="导致偏差和方差的原因"><a href="#导致偏差和方差的原因" class="headerlink" title="导致偏差和方差的原因"></a>导致偏差和方差的原因</h3><ul>
<li><strong>偏差</strong>通常是由于我们对学习算法做了<strong>错误的假设</strong>，或者模型的复杂度不够；<ul>
<li>比如真实模型是一个二次函数，而我们假设模型为一次函数，这就会导致偏差的增大（欠拟合）；</li>
<li><strong>由偏差引起的误差</strong>通常在<strong>训练误差</strong>上就能体现，或者说训练误差主要是由偏差造成的</li>
</ul>
</li>
<li><strong>方差</strong>通常是由于<strong>模型的复杂度相对于训练集过高</strong>导致的；<ul>
<li>比如真实模型是一个简单的二次函数，而我们假设模型是一个高次函数，这就会导致方差的增大（过拟合）；</li>
<li><strong>由方差引起的误差</strong>通常体现在测试误差相对训练误差的<strong>增量</strong>上。</li>
</ul>
</li>
</ul>
<h3 id="深度学习中的偏差与方差"><a href="#深度学习中的偏差与方差" class="headerlink" title="深度学习中的偏差与方差"></a>深度学习中的偏差与方差</h3><ul>
<li>神经网络的拟合能力非常强，因此它的<strong>训练误差</strong>（偏差）通常较小；</li>
<li>但是过强的拟合能力会导致较大的方差，使模型的测试误差（<strong>泛化误差</strong>）增大；</li>
<li>因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为<strong>正则化方法</strong>。<blockquote>
<p>../深度学习/<a href="../A-深度学习/C-专题-正则化">正则化</a></p>
</blockquote>
</li>
</ul>
<h3 id="偏差-方差-与-Boosting-Bagging"><a href="#偏差-方差-与-Boosting-Bagging" class="headerlink" title="偏差/方差 与 Boosting/Bagging"></a>偏差/方差 与 Boosting/Bagging</h3><blockquote>
<p>./集成学习专题/<a href="./C-专题-集成学习#boostingbagging-与-偏差方差-的关系">Boosting/Bagging 与 偏差/方差 的关系</a></p>
</blockquote>
<h3 id="偏差与方差的计算公式"><a href="#偏差与方差的计算公式" class="headerlink" title="偏差与方差的计算公式"></a>偏差与方差的计算公式</h3><ul>
<li><p>记在<strong>训练集 D</strong> 上学得的模型为</p>
<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=f(\boldsymbol{x};D)"><img src="/2022/03/02/ML1/公式_20180817211749.png" height></a></div>

<p>模型的<strong>期望预测</strong>为</p>
<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;\hat{f}(\boldsymbol{x})=\mathbb{E}_D[f(\boldsymbol{x};D)]"><img src="/2022/03/02/ML1/公式_20180817210758.png" height></a></div>
</li>
<li><p><strong>偏差</strong>（Bias）</p>
<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;bias^2(\boldsymbol{x})=(\hat{f}(\boldsymbol{x})-y)^2"><img src="/2022/03/02/ML1/公式_20180817210106.png" height></a></div>

<blockquote>
<p><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；</p>
</blockquote>
</li>
<li><p><strong>方差</strong>（Variance）</p>
<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;var(\boldsymbol{x})=\mathbb{E}_D\left&space;[&space;\left&space;(&space;f(\boldsymbol{x};D)-\hat{f}(\boldsymbol{x})&space;\right&space;)^2&space;\right&space;]"><img src="/2022/03/02/ML1/公式_20180817211903.png" height></a></div>

<blockquote>
<p><strong>方差</strong>度量了同样大小的<strong>训练集的变动</strong>所导致的学习性能的变化，即刻画了数据扰动所造成的影响（模型的稳定性）；</p>
<!-- - **噪声**
<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;var(\boldsymbol{x})=\mathbb{E}_D\left&space;[&space;\left&space;(&space;f(\boldsymbol{x};D)-\hat{f}(\boldsymbol{x})&space;\right&space;)^2&space;\right&space;]"><img src="ML/公式_20180817212111.png" height="" /></a></div> -->
</blockquote>
</li>
<li><p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
</li>
<li><p>“<strong>偏差-方差分解</strong>”表明模型的泛化能力是由算法的能力、数据的充分性、任务本身的难度共同决定的。</p>
</li>
</ul>
<h3 id="偏差与方差的权衡（过拟合与模型复杂度的权衡）"><a href="#偏差与方差的权衡（过拟合与模型复杂度的权衡）" class="headerlink" title="偏差与方差的权衡（过拟合与模型复杂度的权衡）"></a>偏差与方差的权衡（过拟合与模型复杂度的权衡）</h3><ul>
<li><p>给定学习任务，</p>
<ul>
<li>当训练不足时，模型的<strong>拟合能力不够</strong>（数据的扰动不足以使模型产生显著的变化），此时<strong>偏差</strong>主导模型的泛化误差；</li>
<li>随着训练的进行，模型的<strong>拟合能力增强</strong>（模型能够学习数据发生的扰动），此时<strong>方差</strong>逐渐主导模型的泛化误差；</li>
<li>当训练充足后，模型的<strong>拟合能力过强</strong>（数据的轻微扰动都会导致模型产生显著的变化），此时即发生<strong>过拟合</strong>（训练数据自身的、非全局的特征也被模型学习了）</li>
</ul>
</li>
<li><p>偏差和方差的关系和<strong>模型容量</strong>（模型复杂度）、<strong>欠拟合</strong>和<strong>过拟合</strong>的概念紧密相联</p>
<div align="center"><img src="/2022/03/02/ML1/TIM截图20180817214034.png" height></div>

<ul>
<li>当模型的容量增大（x 轴）时， 偏差（用点表示）随之减小，而方差（虚线）随之增大</li>
<li>沿着 x 轴存在<strong>最佳容量</strong>，<strong>小于最佳容量会呈现欠拟合</strong>，<strong>大于最佳容量会导致过拟合</strong>。<blockquote>
<p>《深度学习》 5.4.4 权衡偏差和方差以最小化均方误差</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><strong>Reference</strong></p>
<ul>
<li><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Understanding the Bias-Variance Tradeoff</a></li>
<li><a href="https://www.zhihu.com/question/27068705">机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？</a> - 知乎 </li>
</ul>
<h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><blockquote>
<p>《统计学习方法》 1.7 生成模型与判别模型</p>
<ul>
<li>监督学习的任务是学习一个模型，对给定的输入预测相应的输出</li>
<li>这个模型的一般形式为一个<strong>决策函数</strong>或一个<strong>条件概率分布</strong>（后验概率）：<div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\fn_phv&space;\large&space;Y=f(X)\quad&space;\text{or}\quad&space;P(Y|X)"><img src="/2022/03/02/ML1/公式_20180817220004.png" height></a></div>

</li>
</ul>
</blockquote>
<ul>
<li><strong>决策函数</strong>：输入 X 返回 Y；其中 Y 与一个<strong>阈值</strong>比较，然后根据比较结果判定 X 的类别</li>
<li><strong>条件概率分布</strong>：输入 X 返回 <strong>X 属于每个类别的概率</strong>；将其中概率最大的作为 X 所属的类别<ul>
<li>监督学习模型可分为<strong>生成模型</strong>与<strong>判别模型</strong></li>
</ul>
</li>
<li><strong>判别模型</strong>直接学习决策函数或者条件概率分布<ul>
<li>直观来说，<strong>判别模型</strong>学习的是类别之间的最优分隔面，反映的是不同类数据之间的差异</li>
</ul>
</li>
<li><strong>生成模型</strong>学习的是联合概率分布<code>P(X,Y)</code>，然后根据条件概率公式计算 <code>P(Y|X)</code><div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\fn_phv&space;\large&space;P(Y|X)=\frac{P(X,Y)}{P(X)}"><img src="/2022/03/02/ML1/公式_20180817223923.png" height></a></div>

</li>
</ul>
<p><strong>两者之间的联系</strong></p>
<ul>
<li>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</li>
<li>当存在“<strong>隐变量</strong>”时，只能使用<strong>生成模型</strong><blockquote>
<p>隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”</p>
</blockquote>
</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li><strong>判别模型</strong><ul>
<li>优点<ul>
<li>直接面对预测，往往学习的准确率更高</li>
<li>由于直接学习 <code>P(Y|X)</code> 或 <code>f(X)</code>，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程</li>
</ul>
</li>
<li>缺点<ul>
<li>不能反映训练数据本身的特性</li>
<li>…</li>
</ul>
</li>
</ul>
</li>
<li><strong>生成模型</strong><ul>
<li>优点<ul>
<li>可以还原出联合概率分布 <code>P(X,Y)</code>，判别方法不能</li>
<li>学习收敛速度更快——即当样本容量增加时，学到的模型可以更快地收敛到真实模型</li>
<li>当存在“隐变量”时，只能使用生成模型</li>
</ul>
</li>
<li>缺点<ul>
<li>学习和计算过程比较复杂</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>常见模型</strong></p>
<ul>
<li>判别模型<ul>
<li>K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、<strong>最大熵模型</strong>、SVM、提升方法、<strong>条件随机场</strong></li>
</ul>
</li>
<li>生成模型<ul>
<li>朴素贝叶斯、隐马尔可夫模型、混合高斯模型、贝叶斯网络、马尔可夫随机场</li>
</ul>
</li>
</ul>
<p><strong>Reference</strong></p>
<ul>
<li><a href="https://blog.csdn.net/u012101561/article/details/52814571">机器学习—-生成模型与判别模型</a> - CSDN博客 </li>
<li></li>
</ul>
<h2 id="先验概率与后验概率"><a href="#先验概率与后验概率" class="headerlink" title="先验概率与后验概率"></a>先验概率与后验概率</h2><blockquote>
<p><a href="https://blog.csdn.net/suranxu007/article/details/50326873">先验概率，后验概率，似然概率，条件概率，贝叶斯，最大似然</a> - CSDN博客 </p>
</blockquote>
<p><strong>条件概率</strong>（似然概率）</p>
<ul>
<li>一个事件发生后另一个事件发生的概率。</li>
<li>一般的形式为 <code>P(X|Y)</code>，表示 y 发生的条件下 x 发生的概率。</li>
<li>有时为了区分一般意义上的<strong>条件概率</strong>，也称<strong>似然概率</strong></li>
</ul>
<p><strong>先验概率</strong></p>
<ul>
<li>事件发生前的预判概率</li>
<li>可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。</li>
<li>一般都是<strong>单独事件</strong>发生的概率，如 <code>P(A)</code>、<code>P(B)</code>。</li>
</ul>
<p><strong>后验概率</strong></p>
<ul>
<li>基于先验概率求得的<strong>反向条件概率</strong>，形式上与条件概率相同（若 <code>P(X|Y)</code> 为正向，则 <code>P(Y|X)</code> 为反向）</li>
</ul>
<p><strong>贝叶斯公式</strong><br>  <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\fn_phv&space;\large&space;P(Y|X)=\frac{P(X|Y)*P(Y)}{P(X)}"><img src="/2022/03/02/ML1/公式_20180817230314.png" height></a></div></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title>个性化推荐系统与强化学习</title>
    <url>/2022/03/03/RLRe/</url>
    <content><![CDATA[<p>通过融合深度学习与知识图谱技术，推荐系统的性能取得了大幅的提升。然而，多数的推荐系统仍是以一步到位的方式建立的：它们有着类似的搭建方式，即在充分获取用户历史数据的前提下，设计并训练特定的监督模型，从而得到用户对于不同物品的喜好程度。这些训练好的模型在部署上线后可以为特定用户识别出最具吸引力的物品，为其做出个性化推荐。在此，人们往往假设用户数据已充分获取，且其行为会在较长时间之内保持稳定，使得上述过程中所建立的推荐模型得以应付实际中的需求。</p>
<p>然而对于诸多现实场景，例如电子商务或者在线新闻平台，用户与推荐系统之间往往会发生持续密切的交互行为。在这一过程中，用户的反馈将弥补可能的数据缺失，同时有力地揭示其当前的行为特征，从而为系统进行更加精准的个性化推荐提供重要的依据。</p>
<p>强化学习为解决这个问题提供了有力支持。依照用户的行为特征，我们将涉及到的推荐场景划分为静态与动态，并分别对其进行讨论。</p>
<h1 id="静态场景下的强化学习"><a href="#静态场景下的强化学习" class="headerlink" title="静态场景下的强化学习"></a>静态场景下的强化学习</h1><p>在静态场景之下，用户的行为特征在与系统的交互过程中保持稳定不变。对于这一场景，一类有代表性的工作是基于上下文多臂老虎机（contextual multi-armed bandit）的推荐系统，它的发展为克服推荐场景中的冷启动问题提供了行之有效的解决方案。</p>
<p>在许多现实应用中，用户的历史行为往往服从特定的长尾分布，即大多数用户仅仅产生规模有限的历史数据，而极少的用户则会生成较为充足的历史数据。这一现象所带来的数据稀疏问题使得传统模型在很多时候难以得到令人满意的实际效果。</p>
<p>为此，一个直接的应对方法是对用户行为进行主动式的探索，即通过对用户发起大量尝试性的推荐，以充分的获得其行为数据，从而保障推荐系统的可用性。然而不幸的是，这一简单的做法势必引发极大的探索开销，使得它在现实中并不具备可行性。</p>
<p>为使主动式探索具备可行的效用开销，人们尝试借助多臂老虎机问题所带来的启发。多臂老虎机问题旨在于“探索-利用”间做出最优的权衡，为此诸多经典算法被相继提出。尽管不同的算法有着不同的实施机制，它们的设计都本着一个共同的原则。</p>
<p>具体说来，系统在做出推荐的时候会综合考虑物品的推荐效用以及累积尝试。较高的推荐效用预示着较低的探索开销，而较低的累积尝试则表明较高的不确定性。为此，不同的算法都会设计特定的整合机制，使得同时具备较高推荐效用与不确定性物品可以得到优先尝试。</p>
<h1 id="动态场景下的强化学习"><a href="#动态场景下的强化学习" class="headerlink" title="动态场景下的强化学习"></a>动态场景下的强化学习</h1><p>在多臂老虎机的设定场景下，用户的实时特征被假设为固定不变的，因此算法并未涉及用户行为发生动态迁移的情况。然而对于诸多现实中的推荐场景，用户行为往往会在交互过程中不断变化。这就要求推荐系统依照用户反馈精确估计其状态发展，并为之制定优化的推荐策略。</p>
<p>具体来讲，一个理想的推荐系统应满足如下双方面的属性。一方面，推荐决策需要充分基于用户过往的反馈数据；另一方面，推荐系统需要优化整个交互过程之中的全局收益。强化学习为实现上述目标提供了有力的技术支持。</p>
<p>在强化学习的框架之下，推荐系统被视作一个智能体（agent），用户当前的行为特征被抽象成为状态（state），待推荐的对象（如候选新闻）则被当作动作（action）。在每次推荐交互中，系统依据用户的状态，选择合适的动作，以最大化特定的长效目标（如点击总数或停留时长）。推荐系统与用户交互过程中所产生的行为数据被组织成为经验（experience），用以记录相应动作产生的奖励（reward）以及状态转移（state-transition）。基于不断积累的经验，强化学习算法得出策略（policy），用以指导特定状态下最优的动作选取。   </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 日记：日常练习篇</title>
    <url>/2022/03/01/Leetcode2/</url>
    <content><![CDATA[<h1 id="179-最大数"><a href="#179-最大数" class="headerlink" title="179. 最大数"></a>179. 最大数</h1><h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><p>给定一组非负整数 nums，重新排列每个数的顺序（每个数不可拆分）使之组成一个最大的整数。</p>
<p>注意：输出结果可能非常大，所以你需要返回一个字符串而不是整数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">cmp</span> <span class="params">(<span class="keyword">int</span> &amp;a, <span class="keyword">int</span> &amp;b)</span></span>&#123;</span><br><span class="line">            <span class="built_in">string</span> aa = to_string(a) + to_string(b);</span><br><span class="line">            <span class="built_in">string</span> bb = to_string(b) + to_string(a);</span><br><span class="line">            <span class="keyword">return</span> aa &gt; bb;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">largestNumber</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> ans;</span><br><span class="line">        sort(nums.begin(), nums.end(), cmp);</span><br><span class="line">        <span class="keyword">if</span>(nums[<span class="number">0</span>] == <span class="number">0</span>) <span class="keyword">return</span> <span class="string">&quot;0&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;n : nums)&#123;</span><br><span class="line">            ans += to_string(n);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="769-最多能完成排序的块"><a href="#769-最多能完成排序的块" class="headerlink" title="769. 最多能完成排序的块"></a>769. 最多能完成排序的块</h1><h2 id="题目描述：-1"><a href="#题目描述：-1" class="headerlink" title="题目描述："></a>题目描述：</h2><p>给定一个长度为 n 的整数数组 arr ，它表示在 [0, n - 1] 范围内的整数的排列。</p>
<p>我们将 arr 分割成若干 块 (即分区)，并对每个块单独排序。将它们连接起来后，使得连接的结果和按升序排序后的原数组相同。</p>
<p>返回数组能分成的最多块数量。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxChunksToSorted</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = arr.size();</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>, max_value = arr[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            max_value = max(arr[i], max_value);</span><br><span class="line">            <span class="keyword">if</span>(max_value == i)&#123;</span><br><span class="line">                ++res;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="739-每日温度"><a href="#739-每日温度" class="headerlink" title="739. 每日温度"></a>739. 每日温度</h1><h2 id="题目描述：-2"><a href="#题目描述：-2" class="headerlink" title="题目描述："></a>题目描述：</h2><p>给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指在第 i 天之后，才会有更高的温度。如果气温在这之后都不会升高，请在该位置用 0 来代替。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dailyTemperatures</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; temperatures)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = temperatures.size();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>)  <span class="keyword">return</span> &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; ss;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">res</span><span class="params">(n)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!ss.empty())&#123;</span><br><span class="line">                <span class="keyword">int</span> pre_index = ss.top();</span><br><span class="line">                <span class="keyword">if</span>(temperatures[i] &lt;= temperatures[pre_index])&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                ss.pop();</span><br><span class="line">                res[pre_index] = i - pre_index;</span><br><span class="line">            &#125;</span><br><span class="line">            ss.push(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="218-天际线问题"><a href="#218-天际线问题" class="headerlink" title="218. 天际线问题"></a>218. 天际线问题</h1><p>城市的 天际线 是从远处观看该城市中所有建筑物形成的轮廓的外部轮廓。给你所有建筑物的位置和高度，请返回 由这些建筑物形成的 天际线 。</p>
<p>每个建筑物的几何信息由数组 buildings 表示，其中三元组 buildings[i] = [lefti, righti, heighti] 表示：</p>
<p>lefti 是第 i 座建筑物左边缘的 x 坐标。<br>righti 是第 i 座建筑物右边缘的 x 坐标。<br>heighti 是第 i 座建筑物的高度。<br>你可以假设所有的建筑都是完美的长方形，在高度为 0 的绝对平坦的表面上。</p>
<p>天际线 应该表示为由 “关键点” 组成的列表，格式 [[x1,y1],[x2,y2],…] ，并按 x 坐标 进行 排序 。关键点是水平线段的左端点。列表中最后一个点是最右侧建筑物的终点，y 坐标始终为 0 ，仅用于标记天际线的终点。此外，任何两个相邻建筑物之间的地面都应被视为天际线轮廓的一部分。</p>
<p>注意：输出天际线中不得有连续的相同高度的水平线。例如 […[2 3], [4 5], [7 5], [11 5], [12 7]…] 是不正确的答案；三条高度为 5 的线应该在最终输出中合并为一个：[…[2 3], [4 5], [12 7], …]</p>
<p>1.判断高度最高的矩阵和当前矩阵是否重合，最高矩阵的右端点大于等于当前矩阵的左端点那么两个矩阵有重合。<br>2.使用优先级队列存储矩阵高度，当矩阵重合时，方便选择端点的最高高度<br>3.处理左端点，将当前矩阵的高度入队，选择左端点和最高高度组成左天际线<br>4.处理右端点，选择最高高度的矩阵的右端点，及其右侧的重合的矩阵（不包括本矩阵）最高高度，组成右天际线</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">getSkyline</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; buildings)</span> </span>&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line"><span class="built_in">priority_queue</span>&lt;<span class="built_in">pair</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; max_heap; </span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>, len = buildings.size();</span><br><span class="line"><span class="keyword">int</span> cur_x, cur_h;</span><br><span class="line"><span class="keyword">while</span> (i &lt; len || !max_heap.empty()) &#123;</span><br><span class="line">    <span class="comment">//如果最高的矩阵和当前矩阵重合，处理左端点</span></span><br><span class="line">    <span class="keyword">if</span> (max_heap.empty() || i &lt; len &amp;&amp; buildings[i][<span class="number">0</span>] &lt;= max_heap.top().second) &#123;</span><br><span class="line">        cur_x = buildings[i][<span class="number">0</span>];</span><br><span class="line">        <span class="comment">//相同的左端点全部入队，选择最高高度</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; len &amp;&amp; cur_x == buildings[i][<span class="number">0</span>]) &#123;</span><br><span class="line">            max_heap.emplace(buildings[i][<span class="number">2</span>], buildings[i][<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//遍历矩阵</span></span><br><span class="line">            ++i;</span><br><span class="line">        &#125; </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//如果最高的矩阵和当前矩阵不重合，处理之前重合矩阵的右端点</span></span><br><span class="line">        cur_x = max_heap.top().second;</span><br><span class="line">        <span class="comment">//选择其右侧重合的矩阵（不包括本矩阵）最高高度</span></span><br><span class="line">        <span class="keyword">while</span> (!max_heap.empty() &amp;&amp; cur_x &gt;= max_heap.top().second) &#123;</span><br><span class="line">            max_heap.pop();</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">    cur_h = (max_heap.empty()) ? <span class="number">0</span> : max_heap.top().first;</span><br><span class="line">    <span class="keyword">if</span> (ans.empty() || cur_h != ans.back()[<span class="number">1</span>]) &#123;</span><br><span class="line">        ans.push_back(&#123;cur_x, cur_h&#125;);</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="303-区域和检索-数组不可变"><a href="#303-区域和检索-数组不可变" class="headerlink" title="303. 区域和检索 - 数组不可变"></a>303. 区域和检索 - 数组不可变</h1><p>给定一个整数数组  nums，处理以下类型的多个查询:</p>
<p>计算索引 left 和 right （包含 left 和 right）之间的 nums 元素的 和 ，其中 left &lt;= right<br>实现 NumArray 类：</p>
<p>NumArray(int[] nums) 使用数组 nums 初始化对象<br>int sumRange(int i, int j) 返回数组 nums 中索引 left 和 right 之间的元素的 总和 ，包含 left 和 right 两点（也就是 nums[left] + nums[left + 1] + … + nums[right] )</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NumArray</span> &#123;</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; psum;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    NumArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums): psum(nums.size() + <span class="number">1</span>, <span class="number">0</span>) &#123;</span><br><span class="line">        partial_sum(nums.begin(), nums.end(), psum.begin() + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">sumRange</span><span class="params">(<span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> psum[right+<span class="number">1</span>] - psum[left];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="210-课程表-II"><a href="#210-课程表-II" class="headerlink" title="210. 课程表 II"></a>210. 课程表 II</h1><p>现在你总共有 numCourses 门课需要选，记为 0 到 numCourses - 1。给你一个数组 prerequisites ，其中 prerequisites[i] = [ai, bi] ，表示在选修课程 ai 前 必须 先选修 bi 。</p>
<p>例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示：[0,1] 。<br>返回你为了学完所有课程所安排的学习顺序。可能会有多个正确的顺序，你只要返回 任意一种 就可以了。如果不可能完成所有课程，返回 一个空数组 。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">findOrder</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">graph</span><span class="params">(numCourses, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;())</span></span>;</span><br><span class="line">        vector&lt;int&gt; indegree(numCourses, 0), res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">auto</span> &amp; prerequisite: prerequisites)&#123;</span><br><span class="line">            graph[prerequisite[<span class="number">1</span>]].push_back(prerequisite[<span class="number">0</span>]);</span><br><span class="line">            ++indegree[prerequisite[<span class="number">0</span>]];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; indegree.size(); ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(!indegree[i])&#123;</span><br><span class="line">                q.push(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!q.empty())&#123;</span><br><span class="line">            <span class="keyword">int</span> u = q.front();</span><br><span class="line">            res.push_back(u);</span><br><span class="line">            q.pop();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> v: graph[u])&#123;</span><br><span class="line">                --indegree[v];</span><br><span class="line">                <span class="keyword">if</span>(!indegree[v])&#123;</span><br><span class="line">                    q.push(v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; indegree.size(); ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(indegree[i])</span><br><span class="line">                <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><a href="https://leetcode-cn.com/problems/substring-with-concatenation-of-all-words/">30. 串联所有单词的子串</a></p>
<h2 id="思路：指定移动步长-指定开始位置"><a href="#思路：指定移动步长-指定开始位置" class="headerlink" title="思路：指定移动步长+指定开始位置"></a>思路：指定移动步长+指定开始位置</h2><p>经典滑动窗口模版题，一一把题目中的要求实现即可，目前做过的滑窗题都可以套模版</p>
<ul>
<li>最重要的条件「长度相同」，暗示滑动窗口移动的步长 stride 为 words[0].size()</li>
<li>窗口的长度应为 words.size() * stride</li>
<li>当前窗口的长度计算公式为 right - left + stride</li>
</ul>
<p>需要设定滑动窗口的起始位置，起始位置可选范围为 [0, stride-1]</p>
<p>时间复杂度：O(30 <em> n </em> 30) 遍历起始位置最大 30 次，滑动窗口遍历整个 s 为 n 次，滑窗内 substr 的截取和判断为最大 30 次</p>
<p>空间复杂度：O(n)</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">findSubstring</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; words)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = s.size();</span><br><span class="line">        <span class="keyword">int</span> stride = words[<span class="number">0</span>].size();</span><br><span class="line">        <span class="keyword">int</span> limit = words.size() * stride;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; need;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> w : words) &#123;</span><br><span class="line">            need[w]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> start = <span class="number">0</span>; start &lt;= stride - <span class="number">1</span>; start++) &#123;</span><br><span class="line">            <span class="comment">// left 和 right 指向的是当前步子的第一个下标</span></span><br><span class="line">            <span class="keyword">int</span> left = start;</span><br><span class="line">            <span class="keyword">int</span> right = start;</span><br><span class="line">            <span class="keyword">int</span> cnt = <span class="number">0</span>;  <span class="comment">// 记录窗口内满足要求的单词数量</span></span><br><span class="line">            <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; window;</span><br><span class="line">            <span class="keyword">while</span> (right &lt; n) &#123;</span><br><span class="line">                <span class="comment">// 右边届入窗</span></span><br><span class="line">                <span class="built_in">string</span> cur_right = s.substr(right, stride);</span><br><span class="line">                <span class="keyword">if</span> (need.count(cur_right)) &#123;</span><br><span class="line">                    window[cur_right]++;</span><br><span class="line">                    <span class="keyword">if</span> (window[cur_right] == need[cur_right]) &#123;</span><br><span class="line">                        cnt++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 左边届收缩</span></span><br><span class="line">                <span class="keyword">if</span> (right - left + stride &gt; limit) &#123;</span><br><span class="line">                    <span class="built_in">string</span> cur_left = s.substr(left, stride);</span><br><span class="line">                    <span class="keyword">if</span> (need.count(cur_left)) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (window[cur_left] == need[cur_left]) &#123;</span><br><span class="line">                            cnt--;</span><br><span class="line">                        &#125;</span><br><span class="line">                        window[cur_left]--;</span><br><span class="line">                    &#125;</span><br><span class="line">                    left += stride;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 采集答案</span></span><br><span class="line">                <span class="keyword">if</span> (right - left + stride == limit &amp;&amp; cnt == need.size()) &#123;</span><br><span class="line">                    ans.push_back(left);</span><br><span class="line">                &#125;</span><br><span class="line">                right += stride;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>窗口的数据结构</strong>：应根据各种数据结构的特点来选取</p>
<p>哈希表，unordered_map<br>优先队列，priority_queue<br>红黑树，multiset、set、multimap、map<br>单调队列，deque实现</p>
<p>应用条件：</p>
<p>原数据必须满足单调性，不满足的可以用前缀和。例如560. 和为 K 的子数组原数据中含有负数，破坏了单调性</p>
<p>思考（也是考点）：</p>
<ol>
<li>窗口使用什么数据结构</li>
<li>达到窗口限定后，左边届该怎样收缩，如果无法以常规方法收缩，考虑延时删除策略（在采集答案前进行）</li>
<li>怎样采集答案</li>
<li>滑窗的步长</li>
<li>滑窗的起始位置</li>
<li>是否需要数据预处理，例如排序，才能使用滑窗</li>
</ol>
<h2 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h2><p>注意：可能会在各个地方去采集结果，一般会放在左窗口收缩后，因为此时窗口是满足题目要求的，但要「随机应变」</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">slidingWindow</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">string</span> t)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> right = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> valid = <span class="number">0</span>; <span class="comment">// 窗口内已凑齐的字符种类数量</span></span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; window;</span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; need; <span class="comment">// 需要凑齐的字符和对应数量</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">char</span> c : t) need[c]++;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (right &lt; s.size()) &#123;</span><br><span class="line">        <span class="comment">// 右边届入窗口，进行窗口内数据的一系列更新</span></span><br><span class="line">        window[s[right]]++;</span><br><span class="line">        <span class="keyword">if</span> (window[s[right]] == need[s[right]]) &#123;</span><br><span class="line">            valid++;</span><br><span class="line">        &#125;  <span class="comment">// 注意：先加，再判断</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断左侧窗口是否要收缩</span></span><br><span class="line">        <span class="keyword">while</span> (window needs shrink) &#123;</span><br><span class="line">            <span class="comment">// 左边界移出窗口，进行窗口内数据的一系列更新</span></span><br><span class="line">            <span class="keyword">if</span> (window[s[left]] == need[s[left]]) &#123;</span><br><span class="line">                valid--;</span><br><span class="line">            &#125;  <span class="comment">// 注意：先判断，再减</span></span><br><span class="line">            window[s[left]]--;</span><br><span class="line">            left++;  <span class="comment">// 注意：左边届的收缩，要写在所有处理完成的最后</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 采集答案...</span></span><br><span class="line">        right++;  <span class="comment">// 注意：右边届的扩展，要写在所有处理完成的最后</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Ray：Data</title>
    <url>/2022/04/07/ray_data/</url>
    <content><![CDATA[<h1 id="Ray数据集-分布式数据加载和计算"><a href="#Ray数据集-分布式数据加载和计算" class="headerlink" title="Ray数据集:分布式数据加载和计算"></a>Ray数据集:分布式数据加载和计算</h1><p>Ray数据集是在Ray库和应用程序中加载和交换数据的标准方式。它们提供基本的分布式数据转换，如<code>映射</code>、<code>过滤</code>和<code>重分区</code>，并与各种文件格式、数据源和分布式框架兼容。</p>
<p>Ray数据集简化了通用GPU和CPU并行计算;例如，GPU批处理推理。在这种尴尬的并行计算情况下，它为Ray任务和角色提供了更高层次的API，内部处理批处理、流水线和内存管理等操作。</p>
<p><img src="/2022/04/07/ray_data/dataset.svg" alt="Ray_data_head"></p>
<p>作为Ray生态系统的一部分，Ray数据集可以充分利用Ray的分布式调度器的全部功能，例如，使用actor来优化设置时间和GPU调度。</p>
<h1 id="数据集快速启动"><a href="#数据集快速启动" class="headerlink" title="数据集快速启动"></a>数据集快速启动</h1><p>Ray数据集实现分布式 <a href="https://arrow.apache.org/">Arrow</a>。数据集由Ray对象引用<em>blocks</em>的列表组成。每个<em>block</em>包含<a href="https://arrow.apache.org/docs/python/data.html#tables">Arrow table</a>(创建或转换为表格或张量数据时)、<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">Pandas DataFrame</a>(创建或转换为Pandas数据时)或Python列表(否则)中的一组项。让我们从创建数据集开始。</p>
<h1 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h1><p>运行<code>pip install &quot;ray[data]&quot;</code>开始!</p>
<p>可以从使用<code>ray.data.range()</code>和<code>ray.data.from_items()</code>从合成数据创建数据集开始。数据集既可以保存普通的Python对象(即它们的模式是Python类型)，也可以保存Arrow记录(在这种情况下它们的模式是Arrow)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Dataset of Python objects.</span></span><br><span class="line">ds = ray.data.range(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># -&gt; Dataset(num_blocks=200, num_rows=10000, schema=&lt;class &#x27;int&#x27;&gt;)</span></span><br><span class="line"></span><br><span class="line">ds.take(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># -&gt; [0, 1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line">ds.count()</span><br><span class="line"><span class="comment"># -&gt; 10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Dataset of Arrow records.</span></span><br><span class="line">ds = ray.data.from_items([&#123;<span class="string">&quot;col1&quot;</span>: i, <span class="string">&quot;col2&quot;</span>: str(i)&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>)])</span><br><span class="line"><span class="comment"># -&gt; Dataset(num_blocks=200, num_rows=10000, schema=&#123;col1: int64, col2: string&#125;)</span></span><br><span class="line"></span><br><span class="line">ds.show(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># -&gt; &#123;&#x27;col1&#x27;: 0, &#x27;col2&#x27;: &#x27;0&#x27;&#125;</span></span><br><span class="line"><span class="comment"># -&gt; &#123;&#x27;col1&#x27;: 1, &#x27;col2&#x27;: &#x27;1&#x27;&#125;</span></span><br><span class="line"><span class="comment"># -&gt; &#123;&#x27;col1&#x27;: 2, &#x27;col2&#x27;: &#x27;2&#x27;&#125;</span></span><br><span class="line"><span class="comment"># -&gt; &#123;&#x27;col1&#x27;: 3, &#x27;col2&#x27;: &#x27;3&#x27;&#125;</span></span><br><span class="line"><span class="comment"># -&gt; &#123;&#x27;col1&#x27;: 4, &#x27;col2&#x27;: &#x27;4&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">ds.schema()</span><br><span class="line"><span class="comment"># -&gt; col1: int64</span></span><br><span class="line"><span class="comment"># -&gt; col2: string</span></span><br></pre></td></tr></table></figure>
<p>数据集可以从本地磁盘或远程数据源(如S3)上的文件创建。pyarrow支持的任何文件系统都可以用来指定文件位置:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Read a directory of files in remote storage.</span></span><br><span class="line">ds = ray.data.read_csv(<span class="string">&quot;s3://bucket/path&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read multiple local files.</span></span><br><span class="line">ds = ray.data.read_csv([<span class="string">&quot;/path/to/file1&quot;</span>, <span class="string">&quot;/path/to/file2&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read multiple directories.</span></span><br><span class="line">ds = ray.data.read_csv([<span class="string">&quot;s3://bucket/path1&quot;</span>, <span class="string">&quot;s3://bucket/path2&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>最后，你可以从Ray对象存储或兼容Ray的分布式DataFrames中创建一个数据集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> dask.dataframe <span class="keyword">as</span> dd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Dataset from a list of Pandas DataFrame objects.</span></span><br><span class="line">pdf = pd.DataFrame(&#123;<span class="string">&quot;one&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&quot;two&quot;</span>: [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>]&#125;)</span><br><span class="line">ds = ray.data.from_pandas([pdf])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Dataset from a Dask-on-Ray DataFrame.</span></span><br><span class="line">dask_df = dd.from_pandas(pdf, npartitions=<span class="number">10</span>)</span><br><span class="line">ds = ray.data.from_dask(dask_df)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="保存数据集"><a href="#保存数据集" class="headerlink" title="保存数据集"></a>保存数据集</h1><p>数据集可以使用<code>.write_csv()</code>、<code>.write_json()</code>和<code>.write_parquet()</code>写入本地或远程存储。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Write to csv files in /tmp/output.</span></span><br><span class="line">ray.data.range(<span class="number">10000</span>).write_csv(<span class="string">&quot;/tmp/output&quot;</span>)</span><br><span class="line"><span class="comment"># -&gt; /tmp/output/data0.csv, /tmp/output/data1.csv, ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use repartition to control the number of output files:</span></span><br><span class="line">ray.data.range(<span class="number">10000</span>).repartition(<span class="number">1</span>).write_csv(<span class="string">&quot;/tmp/output2&quot;</span>)</span><br><span class="line"><span class="comment"># -&gt; /tmp/output2/data0.csv</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Transforming-Datasets"><a href="#Transforming-Datasets" class="headerlink" title="Transforming Datasets"></a>Transforming Datasets</h1><p>数据集可以使用<code>.map_batches()</code>进行并行转换。Ray将使用给定的函数转换数据集中的记录批。函数必须返回一批记录。允许您筛选或向批处理中添加其他记录，这将改变数据集的大小。<br>转换被急切地执行并阻塞，直到操作完成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_batch</span>(<span class="params">df: pandas.DataFrame</span>) -&gt; pandas.DataFrame:</span></span><br><span class="line">    <span class="keyword">return</span> df.applymap(<span class="keyword">lambda</span> x: x * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">ds = ray.data.range_arrow(<span class="number">10000</span>)</span><br><span class="line">ds = ds.map_batches(transform_batch, batch_format=<span class="string">&quot;pandas&quot;</span>)</span><br><span class="line"><span class="comment"># -&gt; Map Progress: 100%|████████████████████| 200/200 [00:00&lt;00:00, 1927.62it/s]</span></span><br><span class="line">ds.take(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># -&gt; [&#123;&#x27;value&#x27;: 0&#125;, &#123;&#x27;value&#x27;: 2&#125;, ...]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>批处理格式可以使用batch_format选项指定，默认值为” native “，这意味着支持arrow的批处理使用pandas格式，其他类型使用Python列表。您还可以显式地指定“arrow”或“pandas”来强制转换为该批格式。批量大小也可以选择。如果没有给出，批处理大小将默认为整个块。</p>
<p>数据集还提供了方便的方法map、flat_map和filter，它们不是向量化的(比map_batches慢)，但可能对开发有用。</p>
<p>默认情况下，使用Ray任务执行转换。对于需要设置的转换，指定<code>compute=ray.data.ActorPoolStrategy(min, max)</code>和Ray将使用一个由min到max的角色自动伸缩的角色池来执行你的转换。对于固定大小的actor池，指定<code>ActorPoolStrategy(n, n)</code>。下面是一个使用Ray Data读取、转换和保存批处理推理结果的端到端示例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ray.data <span class="keyword">import</span> ActorPoolStrategy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of GPU batch inference on an ImageNet model.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">images: List[bytes]</span>) -&gt; List[bytes]:</span></span><br><span class="line">    <span class="keyword">return</span> images</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchInferModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model = ImageNetModel()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, batch: pd.DataFrame</span>) -&gt; pd.DataFrame:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(batch)</span><br><span class="line"></span><br><span class="line">ds = ray.data.read_binary_files(<span class="string">&quot;s3://bucket/image-dir&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess the data.</span></span><br><span class="line">ds = ds.map_batches(preprocess)</span><br><span class="line"><span class="comment"># -&gt; Map Progress: 100%|████████████████████| 200/200 [00:00&lt;00:00, 1123.54it/s]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply GPU batch inference with actors, and assign each actor a GPU using</span></span><br><span class="line"><span class="comment"># ``num_gpus=1`` (any Ray remote decorator argument can be used here).</span></span><br><span class="line">ds = ds.map_batches(</span><br><span class="line">    BatchInferModel, compute=ActorPoolStrategy(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">    batch_size=<span class="number">256</span>, num_gpus=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># -&gt; Map Progress (16 actors 4 pending): 100%|██████| 200/200 [00:07, 27.60it/s]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the results.</span></span><br><span class="line">ds.repartition(<span class="number">1</span>).write_json(<span class="string">&quot;s3://bucket/inference-results&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="传递和访问数据集"><a href="#传递和访问数据集" class="headerlink" title="传递和访问数据集"></a>传递和访问数据集</h1><p>数据集可以传递给Ray任务或角色，并通过.iter_batches()或.iter_rows()访问。这不会导致复制，因为数据集的块是作为Ray对象引用传递的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consume</span>(<span class="params">data: Dataset[int]</span>) -&gt; int:</span></span><br><span class="line">    num_batches = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data.iter_batches():</span><br><span class="line">        num_batches += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_batches</span><br><span class="line"></span><br><span class="line">ds = ray.data.range(<span class="number">10000</span>)</span><br><span class="line">ray.get(consume.remote(ds))</span><br><span class="line"><span class="comment"># -&gt; 200</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>数据集可以被分割成不相交的子数据集。如果您向split()函数传递一个actor句柄列表以及所需的分割数量，那么支持位置感知的分割。这是一个常见的模式，用于在分布式训练参与者之间加载和分割数据:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ray.remote(num_gpus=1)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rank: int</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, shard: ray.data.Dataset[int]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> shard.iter_batches(batch_size=<span class="number">256</span>):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> shard.count()</span><br><span class="line"></span><br><span class="line">workers = [Worker.remote(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>)]</span><br><span class="line"><span class="comment"># -&gt; [Actor(Worker, ...), Actor(Worker, ...), ...]</span></span><br><span class="line"></span><br><span class="line">ds = ray.data.range(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># -&gt; Dataset(num_blocks=200, num_rows=10000, schema=&lt;class &#x27;int&#x27;&gt;)</span></span><br><span class="line"></span><br><span class="line">shards = ds.split(n=<span class="number">16</span>, locality_hints=workers)</span><br><span class="line"><span class="comment"># -&gt; [Dataset(num_blocks=13, num_rows=650, schema=&lt;class &#x27;int&#x27;&gt;),</span></span><br><span class="line"><span class="comment">#     Dataset(num_blocks=13, num_rows=650, schema=&lt;class &#x27;int&#x27;&gt;), ...]</span></span><br><span class="line"></span><br><span class="line">ray.get([w.train.remote(s) <span class="keyword">for</span> w, s <span class="keyword">in</span> zip(workers, shards)])</span><br><span class="line"><span class="comment"># -&gt; [650, 650, ...]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Ray</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Ray：Core</title>
    <url>/2022/04/09/ray_core/</url>
    <content><![CDATA[<h1 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h1><p>本节概述Ray的关键概念。这些原语一起工作，使Ray能够灵活地支持广泛的分布式应用程序。</p>
<h2 id="任务Tasks"><a href="#任务Tasks" class="headerlink" title="任务Tasks"></a>任务Tasks</h2><p>Ray允许任意函数在不同的Python工作器上异步执行。这些异步Ray函数被称为“Tasks”。Ray使Task能够根据cpu、gpu和自定义资源来指定它们的资源需求。集群调度器使用这些资源请求来跨集群分发任务以实现并行执行。</p>
<h2 id="演员Actors"><a href="#演员Actors" class="headerlink" title="演员Actors"></a>演员Actors</h2><p>Actors将Ray API从函数(任务)扩展到类。参与者本质上是一个有状态的worker(或server)。当一个新的actor被实例化时，一个新的worker被创建，并且actor的方法被安排在那个特定的worker上，并且可以访问和改变那个worker的状态。和Task一样，Actor也支持CPU、GPU和自定义资源需求。</p>
<h2 id="对象Object"><a href="#对象Object" class="headerlink" title="对象Object"></a>对象Object</h2><p>在Ray中，Task和Actor创建和计算Object。我们将这些Object称为<code>remote objects</code>，因为它们可以存储在Ray集群中的任何位置，我们使用引用来引用它们。Remote objects缓存在Ray的分布式共享内存对象存储中，集群中的每个节点都有一个对象存储。在集群设置中，remote objects可以存在于一个或多个节点上，与谁持有对象ref无关。</p>
<h2 id="Placement-Groups"><a href="#Placement-Groups" class="headerlink" title="Placement Groups"></a>Placement Groups</h2><p>Placement Groups允许用户自动地跨多个节点保留资源组(即，组调度)。然后，可以使用它们来调度Ray Task和Actor，它们按照位置(PACK)尽可能紧密地打包，或者分散(spread)。位置组通常用于安排Actor，但也支持Task。</p>
<h2 id="环境的依赖关系"><a href="#环境的依赖关系" class="headerlink" title="环境的依赖关系"></a>环境的依赖关系</h2><p>当Ray在远程机器上执行Task和Actor时，它们的环境依赖项(例如Python包、本地文件、环境变量)必须可用来运行代码。为了解决这个问题，你可以(1)使用Ray cluster Launcher提前准备你对集群的依赖，或者(2)使用Ray的运行时环境来动态安装它们。</p>
<h1 id><a href="#" class="headerlink" title="#"></a>#</h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Ray</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 日记：背包问题</title>
    <url>/2022/04/01/Leetcode3/</url>
    <content><![CDATA[<h1 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a>背包问题</h1><p>背包问题是一种组合优化的NP 完全问题：有N 个物品和容量为W 的背包，每个物品都有<br>自己的体积w 和价值v，求拿哪些物品可以使得背包所装下物品的总价值最大。如果限定每种物<br>品只能选择0 个或1 个，则问题称为0-1 背包问题；如果不限定每种物品的数量，则问题称为无<br>界背包问题或完全背包问题。</p>
<p>我们可以用动态规划来解决背包问题。以0-1 背包问题为例。我们可以定义一个二维数组dp<br>存储最大价值，其中dp[i][j] 表示前i 件物品体积不超过j 的情况下能达到的最大价值。在我们遍<br>历到第i 件物品时，在当前背包总容量为j 的情况下，如果我们不将物品i 放入背包，那么dp[i][j]<br>= dp[i-1][j]，即前i 个物品的最大价值等于只取前i-1 个物品时的最大价值；如果我们将物品i 放<br>入背包，假设第i 件物品体积为w，价值为v，那么我们得到dp[i][j] = dp[i-1][j-w] + v。我们只需在遍历过程中对这两种情况取最大值即可，总时间复杂度和空间复杂度都为O(NW)。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">knapsack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> N, <span class="keyword">int</span> W)</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(N + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(W + <span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; ++i)&#123;</span><br><span class="line">        <span class="keyword">int</span> w = weights[i<span class="number">-1</span>], v = values[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= W; ++j)&#123;</span><br><span class="line">            <span class="keyword">if</span>(j &gt;= w)&#123;</span><br><span class="line">                dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j-w] + v);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">            &#125;        </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[N][W];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2022/04/01/Leetcode3/bb1.PNG" alt="bb1"></p>
<p>我们可以进一步对0-1 背包进行空间优化，将空间复杂度降低为O(W)。如图所示，假设我们目前考虑物品i = 2，且其体积为w = 2，价值为v = 3；对于背包容量j，我们可以得到dp[2][j] = max(dp[1][j], dp[1][j-2] + 3)。这里可以发现我们永远只依赖于上一排i = 1 的信息，之前算过的其他物品都不需要再使用。因此我们可以去掉dp 矩阵的第一个维度，在考虑物品i 时变成dp[j] = max(dp[j], dp[j-w] + v)。这里要注意我们在遍历每一行的时候必须逆向遍历，这样才能够调用上一行物品i-1 时dp[j-w] 的值；若按照从左往右的顺序进行正向遍历，则dp[j-w] 的值在遍历到 j 之前就已经被更新成物品i 的值了。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">knapsack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> N, <span class="keyword">int</span> W)</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(W + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; ++i)&#123;</span><br><span class="line">        <span class="keyword">int</span> w = weights[i<span class="number">-1</span>], v = values[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = W; j &gt;= w; --j)&#123;</span><br><span class="line">            dp[j] = max(dp[j], dp[j-w] + v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[W];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2022/04/01/Leetcode3/bb2.PNG" alt="bb2"></p>
<p>在完全背包问题中，一个物品可以拿多次。如图上半部分所示，假设我们遍历到物品i = 2，<br>且其体积为w = 2，价值为v = 3；对于背包容量j = 5，最多只能装下2 个该物品。那么我们的状<br>态转移方程就变成了dp[2][5] = max(dp[1][5], dp[1][3] + 3, dp[1][1] + 6)。如果采用这种方法，假设<br>背包容量无穷大而物体的体积无穷小，我们这里的比较次数也会趋近于无穷大，远超O(NW)的<br>时间复杂度。<br>怎么解决这个问题呢？我们发现在dp[2][3] 的时候我们其实已经考虑了dp[1][3] 和dp[2][1]<br>的情况，而在时dp[2][1] 也已经考虑了dp[1][1] 的情况。因此，如图下半部分所示，对于拿多个<br>物品的情况，我们只需考虑dp[2][3] 即可，即dp[2][5] = max(dp[1][5], dp[2][3] + 3)。这样，我们<br>就得到了完全背包问题的状态转移方程：dp[i][j] = max(dp[i-1][j], dp[i][j-w] + v)，其与0-1 背包问<br>题的差别仅仅是把状态转移方程中的第二个i-1 变成了i。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">knapsack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> N, <span class="keyword">int</span> W)</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(N+<span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(W+<span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; ++i)&#123;</span><br><span class="line">        <span class="keyword">int</span> w = weights[i<span class="number">-1</span>], v = velues[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= W; ++j)&#123;</span><br><span class="line">            <span class="keyword">if</span>(j &gt;= w)&#123;</span><br><span class="line">                dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j-w] + v);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[N][W];</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，我们也可以利用空间压缩将时间复杂度降低为O(W)。这里要注意我们在遍历每一<br>行的时候必须正向遍历，因为我们需要利用当前物品在第j-w 列的信息。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">knapsack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> N, <span class="keyword">int</span> W)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(W + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; ++i) &#123;</span><br><span class="line">        <span class="keyword">int</span> w = weights[i<span class="number">-1</span>], v = values[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = w; j &lt;= W; ++j) &#123;</span><br><span class="line">            dp[j] = max(dp[j], dp[j-w] + v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[W];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习面试问题</title>
    <url>/2022/01/03/deep/</url>
    <content><![CDATA[<h1 id="为什么要用深度召回"><a href="#为什么要用深度召回" class="headerlink" title="为什么要用深度召回?"></a>为什么要用深度召回?</h1><p>特征表征能力更强，比如引入了序列特征，位置特征，进行了embedding，加了attention等等</p>
<p>优秀的网络结构+drop机制，相当于构造了特征交叉和无数个自模型进行minibatch上最好的拟合</p>
<p>通过全商品集合上的向量相似检索的方式一定程度上增加召回结果的多样性</p>
<h1 id="dropout如何作用的？"><a href="#dropout如何作用的？" class="headerlink" title="dropout如何作用的？"></a>dropout如何作用的？</h1><p>以p的概率随机的丢掉一些神经元，使得该条链路上的数据不参与loss计算及反向传播。剩余的元素需要除以1-p，保证dropout前后的代价一致</p>
<h1 id="L1为什么在深度学习中不常用？"><a href="#L1为什么在深度学习中不常用？" class="headerlink" title="L1为什么在深度学习中不常用？"></a>L1为什么在深度学习中不常用？</h1><p>L1和L2正则化，在训练的时候限制权值变大；都是针对模型中参数过大的问题引入惩罚项，依据是奥克姆剃刀原理。 在深度学习中，L1会趋向于产生少量的特征，而其他的特征都是0增加网络稀疏性；而L2会选择更多的特征，这些特征都会接近于0，防止过拟合。 神经网络需要每一层的神经元尽可能的提取出有意义的特征，而这些特征不能是无源之水，因此L2正则用的多一些。</p>
<h1 id="用贝叶斯机率说明Dropout的原理？"><a href="#用贝叶斯机率说明Dropout的原理？" class="headerlink" title="用贝叶斯机率说明Dropout的原理？"></a>用贝叶斯机率说明Dropout的原理？</h1><p>通过dropout实现指数级别上的继承父神经网络参数的不同子集模型，使得在有限参数下代表指数数量的子模型成为可能。 单个子模型在面对指定的场景下能有非常好的表现，参数共享又使得剩余的子网络有一个比较好的参数设定。</p>
<h1 id="为什么dropout有效？"><a href="#为什么dropout有效？" class="headerlink" title="为什么dropout有效？"></a>为什么dropout有效？</h1><p>dropout使得网络不至于过于复杂，一定避免了减少了单次的参数量降低过拟合的风险，提升了训练速度</p>
<p>每次dropout相当于生成了一个参数共享的子网络，子网络的个数是指数级别的相当于训练了无数个子模型</p>
<p>每次dropout相当于生成了一个参数共享的子网络，子网络带来的反向传递的结果可以作用在剩余网络上，可被记忆</p>
<p>dropout之后，可以通过修改input不改网络得到同样的计算结果，换句话说就是增加样本</p>
<p>在非线性问题上，通过学习若干个局部空间的特征会比在全局上寻找学习一个整个空间的特征要好，而通过dropout构造样本的稀疏性，来增加特征的区分度。</p>
<h1 id="你觉得bn过程是什么样的？"><a href="#你觉得bn过程是什么样的？" class="headerlink" title="你觉得bn过程是什么样的？"></a>你觉得bn过程是什么样的？</h1><p>1.按batch进行期望和标准差计算<br>2.对整体数据进行标准化<br>3.对标准化的数据进行线性变换<br>变换系数需要学习</p>
<h1 id="手写一下bn过程？"><a href="#手写一下bn过程？" class="headerlink" title="手写一下bn过程？"></a>手写一下bn过程？</h1><h1 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h1><h1 id="残差网络残差作用"><a href="#残差网络残差作用" class="headerlink" title="残差网络残差作用"></a>残差网络残差作用</h1><ol>
<li>防止梯度消失</li>
<li>恒等映射使得网络突破层数限制，避免网络退化</li>
<li>对输出的变化更敏感<br>X=5;F(X)=5.1;F(X)=H(X)+X=&gt;H(X)=0.1<br>X=5;F(X)=5.2;F(X)=H(X)+X=&gt;H(X)=0.2<br>H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化</li>
</ol>
<h1 id="Attention对比RNN和CNN，分别有哪点你觉得的优势？"><a href="#Attention对比RNN和CNN，分别有哪点你觉得的优势？" class="headerlink" title="Attention对比RNN和CNN，分别有哪点你觉得的优势？"></a>Attention对比RNN和CNN，分别有哪点你觉得的优势？</h1><p>对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向<br>对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序</p>
<h1 id="LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失"><a href="#LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失" class="headerlink" title="LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失?"></a>LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失?</h1><h1 id="讲一下pooling的作用，-为什么max-pooling要更常用？哪些情况下，average-pooling比max-pooling更合适？"><a href="#讲一下pooling的作用，-为什么max-pooling要更常用？哪些情况下，average-pooling比max-pooling更合适？" class="headerlink" title="讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？"></a>讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？</h1><h1 id="梯度消失和梯度爆炸的原因是什么？-有哪些解决方法？"><a href="#梯度消失和梯度爆炸的原因是什么？-有哪些解决方法？" class="headerlink" title="梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？"></a>梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？</h1><h1 id="CNN和RNN的梯度消失是一样的吗？"><a href="#CNN和RNN的梯度消失是一样的吗？" class="headerlink" title="CNN和RNN的梯度消失是一样的吗？"></a>CNN和RNN的梯度消失是一样的吗？</h1><h1 id="如果训练的神经网络不收敛，可能有哪些原因？"><a href="#如果训练的神经网络不收敛，可能有哪些原因？" class="headerlink" title="如果训练的神经网络不收敛，可能有哪些原因？"></a>如果训练的神经网络不收敛，可能有哪些原因？</h1><p>检查错误：</p>
<p>没有对数据进行归一化</p>
<p>忘记检查输入和输出</p>
<p>没有对数据进行预处理</p>
<p>没有对数据正则化</p>
<p>使用过大的样本</p>
<p>使用不正确的学习率</p>
<p>在输出层使用错误的激活函数</p>
<p>网络中包含坏梯度</p>
<p>初始化权重错误</p>
<p>过深的网络</p>
<p>隐藏单元数量错误</p>
<p><a href="https://www.jiqizhixin.com/articles/2019-08-01">https://www.jiqizhixin.com/articles/2019-08-01</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Animation：Math</title>
    <url>/2023/10/31/Animation_Math/</url>
    <content><![CDATA[<h1 id="Animation"><a href="#Animation" class="headerlink" title="Animation"></a>Animation</h1><ul>
<li>动画是一种信息传递的工具<ul>
<li>美学经常比技术重要</li>
</ul>
</li>
<li>是模型的延伸→连续性<ul>
<li>Represent scene models as a function of time</li>
</ul>
</li>
<li>输出：sequence of images that when viewed sequentially provide a sense of motion<ul>
<li>电影：24FPS</li>
<li>视频：30FPS、29.994FPS</li>
<li>VR：90FPS （不晕的基础要求）</li>
</ul>
</li>
</ul>
<h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><p>最早：狩猎鹿的动画(Shahr-e Sukhteh, Iran 3200 BCE)</p>
<p>圆盘旋转：(Phenakistoscope, 1831)</p>
<p>第一部Film：Edward Muybridge, “Sallie Gardner” (1878)</p>
<p>First Hand-Drawn Feature-Length (&gt;40 mins) Animation：Disney, “Snow White and the Seven Dwarfs” (1937)</p>
<p>First Digital-Computer-Generated Animation：Ivan Sutherland, “Sketchpad” (1963) – Light pen, vector display</p>
<p>Early Computer Animation：Ed Catmull &amp; Frederick Parke, “Computer Animated Faces” (1972)</p>
<p>Digital Dinosaurs!：Jurassic Park (1993)</p>
<p>First CG Feature-Length Film：Pixar, “Toy Story” (1995) （光栅化）</p>
<p>Computer Animation - 10 years ago：Sony Pictures Animation, “Cloudy With a Chance of Meatballs” (2009)</p>
<p>Computer Animation - last year：Walt Disney Animation Studios, “Frozen 2” (2019)</p>
<h1 id="Keyframe-animation关键帧动画"><a href="#Keyframe-animation关键帧动画" class="headerlink" title="Keyframe animation关键帧动画"></a>Keyframe animation关键帧动画</h1><ul>
<li>Animator (e.g. lead animator) creates keyframes 关键帧</li>
<li>Assistant (person or computer) creates in-between frames (“tweening”) 渐变帧</li>
</ul>
<h2 id="关键的技术难点-Interpolation-插值"><a href="#关键的技术难点-Interpolation-插值" class="headerlink" title="关键的技术难点 - Interpolation 插值"></a>关键的技术难点 - Interpolation 插值</h2><ul>
<li>Linear interpolation usually not good enough</li>
<li>Recall splines for smooth / controllable interpolation</li>
</ul>
<p>B样条……</p>
<h1 id="Physical-Simulation物理模拟"><a href="#Physical-Simulation物理模拟" class="headerlink" title="Physical Simulation物理模拟"></a>Physical Simulation物理模拟</h1><p>模拟、仿真：推导、实现公式，模拟出物体应该怎么变化</p>
<p>例子：布料模拟、流体模拟</p>
<h2 id="质点弹簧系统-Mass-Spring-System-Example-of-Modeling-a-Dynamic-System"><a href="#质点弹簧系统-Mass-Spring-System-Example-of-Modeling-a-Dynamic-System" class="headerlink" title="质点弹簧系统 Mass Spring System: Example of Modeling a Dynamic System"></a>质点弹簧系统 Mass Spring System: Example of Modeling a Dynamic System</h2><p>Example: Mass Spring Rope, Hair, Mass Spring Mesh</p>
<ul>
<li>A Simple Idealized Spring<ul>
<li>没有初始长度</li>
<li>随着拉力线性增长/缩短，线性系数是spring coefficient: stiffness</li>
<li>Force pulls points together</li>
<li>Strength proportional to displacement (Hooke’s Law)</li>
<li>问题：长度会倾向于0</li>
</ul>
</li>
<li><p>Non-Zero Length Spring</p>
<ul>
<li>初始长度Rest length不为零</li>
<li><p>Problem: oscillates forever 永远震荡</p>
<script type="math/tex; mode=display">
\boldsymbol{f}_{a \rightarrow b}=k_{s} \frac{\boldsymbol{b}-\boldsymbol{a}}{\|\boldsymbol{b}-\boldsymbol{a}\|}(\|\boldsymbol{b}-\boldsymbol{a}\|-l)</script></li>
</ul>
</li>
</ul>
<p>Dot Notation for Derivatives：</p>
<script type="math/tex; mode=display">
\begin{aligned} &\boldsymbol{x}\\ &\dot{\boldsymbol{x}}=\boldsymbol{v}\\ &\ddot{\boldsymbol{x}}=\boldsymbol{a} \end{aligned}</script><ul>
<li><p>Introducing Energy Loss</p>
<ul>
<li><p>Simple motion damping 阻尼</p>
<script type="math/tex; mode=display">
  \boldsymbol{f}=-k_{d} \dot{\boldsymbol{b}}</script></li>
<li><p>Behaves like viscous drag on</p>
</li>
<li>Slows down motion in the direction of velocity</li>
<li>$k_d$ is a damping coefficient</li>
<li>问题：Slows down all motion<ul>
<li>Want a rusty spring’s oscillations to slow down, but should it also fall to the ground more slowly? 跟全局速度挂钩</li>
<li>无法表示弹簧内部的损耗</li>
</ul>
</li>
</ul>
</li>
<li><p>Internal Damping for Spring</p>
<p>  <img src="/2023/10/31/Animation_Math/Untitled.png" alt="Animation_Math/Untitled.png"></p>
<ul>
<li>Viscous drag only on change in spring length<ul>
<li>Won’t slow group motion for the spring system (e.g. global translation or rotation of the group)</li>
</ul>
</li>
<li>Note: This is only one specific type of damping 只是一种阻尼的近似</li>
</ul>
</li>
</ul>
<h2 id="Structures-from-Springs"><a href="#Structures-from-Springs" class="headerlink" title="Structures from Springs"></a>Structures from Springs</h2><ul>
<li>Sheets</li>
<li>Blocks</li>
<li>Others<ul>
<li>比如说，一块布的进化</li>
</ul>
</li>
</ul>
<p>Step 1: Sheets</p>
<ul>
<li>This structure will not resist shearing切变会露馅</li>
<li>This structure will not resist out-of-plane bending…</li>
</ul>
<p><img src="/2023/10/31/Animation_Math/Untitled%201.png" alt="Animation_Math/Untitled%201.png"></p>
<p>Step 2: 加强筋</p>
<ul>
<li>This structure will resist shearing but has anisotropic bias 各向异性</li>
<li>This structure will not resist out-of-plane bending either…</li>
</ul>
<p><img src="/2023/10/31/Animation_Math/Untitled%202.png" alt="Animation_Math/Untitled%202.png"></p>
<p>Step 3: 加强筋 plus</p>
<ul>
<li>This structure will resist shearing. Less directional bias.</li>
<li>This structure will not resist out-of-plane bending either… 弯折</li>
</ul>
<p><img src="/2023/10/31/Animation_Math/Untitled%203.png" alt="Animation_Math/Untitled%203.png"></p>
<p>Step 4: 加强筋 max （skip connection）</p>
<ul>
<li>This structure will resist shearing. Less directional bias.</li>
<li>This structure will resist out-of-plane bending （Red springs should be much weaker）</li>
</ul>
<p><img src="/2023/10/31/Animation_Math/Untitled%204.png" alt="Animation_Math/Untitled%204.png"></p>
<h2 id="FEM-Finite-Element-Method-Instead-of-Springs"><a href="#FEM-Finite-Element-Method-Instead-of-Springs" class="headerlink" title="FEM (Finite Element Method) Instead of Springs"></a>FEM (Finite Element Method) Instead of Springs</h2><p>有限元方法</p>
<ul>
<li>车辆碰撞</li>
</ul>
<p>力传导扩散适合用有限元方法建模做</p>
<h1 id="动画系统之Particle-Systems粒子系统"><a href="#动画系统之Particle-Systems粒子系统" class="headerlink" title="动画系统之Particle Systems粒子系统"></a>动画系统之Particle Systems粒子系统</h1><ul>
<li>建模定义很多粒子</li>
<li>每个粒子有自己的属性</li>
</ul>
<p>Model dynamical systems as collections of large numbers of particles </p>
<p>Each particle’s motion is defined by a set of physical (or non-physical) forces </p>
<p>Popular technique in graphics and games</p>
<p>• Easy to understand, implement</p>
<p>• Scalable: fewer particles for speed, more for higher complexity</p>
<p>Challenges</p>
<p>• May need many particles (e.g. fluids)</p>
<p>• May need acceleration structures (e.g. to find nearest particles for interactions)</p>
<p>For each frame in animation</p>
<p>• [If needed] Remove dead particles</p>
<p>• Calculate forces on each particle</p>
<p>• Update each particle’s position and velocity</p>
<p>• [If needed] Create new particles</p>
<p>• Render particles</p>
<p>定义个体和群体之间的关系</p>
<h3 id="Particle-System-Forces"><a href="#Particle-System-Forces" class="headerlink" title="Particle System Forces"></a>Particle System Forces</h3><p>Attraction and repulsion forces</p>
<p>• Gravity, electromagnetism, …</p>
<p>• Springs, propulsion, … </p>
<p>Damping forces</p>
<p>• Friction, air drag, viscosity, … </p>
<p>Collisions</p>
<p>• Walls, containers, fixed objects, …</p>
<p>• Dynamic objects, character body parts, …</p>
<p>星系模拟、Particle-Based Fluids</p>
<p>Example: Simulated Flocking as an ODE</p>
<ul>
<li>定义鸟儿之间交互的规则：个体对群体的观察</li>
<li>Model each bird as a particle Subject to very simple forces:</li>
<li>attraction to center of neighbors</li>
<li>repulsion from individual neighbors</li>
<li>alignment toward average trajectory of neighbors Simulate evolution of large particle system numerically Emergent complex behavior (also seen in fish, bees, …)</li>
</ul>
<p>Example: Molecular Dynamics</p>
<p>Example: Crowds + “Rock” Dynamics</p>
<h1 id="Kinematics"><a href="#Kinematics" class="headerlink" title="Kinematics"></a>Kinematics</h1><p>运动学：正向和反向</p>
<h2 id="Forward-Kinematics-正向运动学"><a href="#Forward-Kinematics-正向运动学" class="headerlink" title="Forward Kinematics 正向运动学"></a>Forward Kinematics 正向运动学</h2><p>明确骨骼之间的运动关系→计算出各个部位的位置</p>
<p>Articulated skeleton</p>
<ul>
<li>Topology (what’s connected to what)</li>
<li>Geometric relations from joints</li>
<li>Tree structure (in absence of loops)</li>
</ul>
<p>Joint types</p>
<ul>
<li>Pin (1D rotation)</li>
<li>Ball (2D rotation)</li>
<li>Prismatic joint (translation)</li>
</ul>
<p>Strengths</p>
<ul>
<li>Direct control is convenient 无法直接控制</li>
<li>Implementation is straightforward</li>
</ul>
<p>Weaknesses</p>
<ul>
<li>Animation may be inconsistent with physics</li>
<li>Time consuming for artists</li>
</ul>
<h2 id="Inverse-Kinematics-逆运动学"><a href="#Inverse-Kinematics-逆运动学" class="headerlink" title="Inverse Kinematics 逆运动学"></a>Inverse Kinematics 逆运动学</h2><p>限制各个部位（通常只有终端）的位置、限制骨骼的运动方式→计算骨骼的运动</p>
<p>方便控制形体整体形状</p>
<p>解特别复杂，可能并不唯一</p>
<p>解法：随机化算法（优化方法，梯度下降）</p>
<p>Numerical solution to general N-link IK problem</p>
<p>• Choose an initial configuration</p>
<p>• Define an error metric (e.g. square of distance between goal and current position)</p>
<p>• Compute gradient of error as function of configuration</p>
<p>• Apply gradient descent (or Newton’s method, or other optimization procedure)</p>
<p>例子：Style-Based IK</p>
<h1 id="Rigging"><a href="#Rigging" class="headerlink" title="Rigging"></a>Rigging</h1><p>对形体的控制，像木偶一样</p>
<p>Rigging is a set of higher level controls on a character that allow more rapid &amp; intuitive modification of pose, deformations, expression, etc.</p>
<p>Important</p>
<p>• Like strings on a puppet</p>
<p>• Captures all meaningful character changes</p>
<p>• Varies from character to character </p>
<p>Expensive to create</p>
<p>• Manual effort 定控制点，拉控制点（应该怎么定、应该怎么拉 → 动画师）</p>
<p>• Requires both artistic and technical training</p>
<h2 id="Blend-Shapes-控制点间的位置插值计算"><a href="#Blend-Shapes-控制点间的位置插值计算" class="headerlink" title="Blend Shapes 控制点间的位置插值计算"></a>Blend Shapes 控制点间的位置插值计算</h2><p>Instead of skeleton, interpolate directly between surfaces</p>
<p>E.g., model a collection of facial expressions:</p>
<p>Simplest scheme: take linear combination of vertex positions</p>
<p>Spline used to control choice of weights over time</p>
<h1 id="Motion-Capture"><a href="#Motion-Capture" class="headerlink" title="Motion Capture"></a>Motion Capture</h1><p>真人控制点反映到虚拟角色中去，需要建立真实和虚拟的联系</p>
<p>Data-driven approach to creating animation sequences</p>
<ul>
<li>Record real-world performances (e.g. person executing an activity)</li>
<li>Extract pose as a function of time from the data collected</li>
</ul>
<p>Strengths</p>
<p>• Can capture large amounts of real data quickly</p>
<p>• Realism can be high </p>
<p>Weaknesses</p>
<p>• Complex and costly set-ups 复杂、花钱</p>
<p>• Captured animation may not meet artistic needs, requiring alterations 不符合艺术家要求，不可能实现的动作</p>
<p>捕捉条件限制</p>
<p>不同的捕捉方法：</p>
<ul>
<li>Optical (More on following slides)<ul>
<li>Markers on subject</li>
<li>Positions by triangulation from multiple cameras</li>
<li>8+ cameras, 240 Hz, occlusions are difficult</li>
</ul>
</li>
<li>Magnetic Sense magnetic fields to infer position / orientation. Tethered.</li>
<li>Mechanical Measure joint angles directly. Restricts motion.</li>
</ul>
<p>很花钱</p>
<p>Data可以可视化成一些曲线</p>
<p><img src="/2023/10/31/Animation_Math/Untitled%205.png" alt="Animation_Math/Untitled%205.png"></p>
<p>Challenges of Facial Animation</p>
<ul>
<li>Uncanny valley<ul>
<li>In robotics and graphics</li>
<li>As artificial character appearance approaches human realism, our emotional response goes negative, until it achieves a sufficiently convincing level of realism in expression</li>
</ul>
</li>
</ul>
<p>Facial Motion Capture </p>
<p>Example：阿凡达</p>
<h1 id="动画的制作流程-The-Production-Pipeline"><a href="#动画的制作流程-The-Production-Pipeline" class="headerlink" title="动画的制作流程 The Production Pipeline"></a>动画的制作流程 The Production Pipeline</h1><p><img src="/2023/10/31/Animation_Math/Untitled%206.png" alt="Animation_Math/Untitled%206.png"></p>
]]></content>
      <categories>
        <category>图形学</category>
      </categories>
      <tags>
        <tag>动画</tag>
      </tags>
  </entry>
  <entry>
    <title>Piccolo:Achitecture</title>
    <url>/2023/10/31/Piccolo/</url>
    <content><![CDATA[<p>Piccolo引擎运行时架构</p>
<p>对应分为平台层 platform 、核心层 core 、资源层 resource 、功能层 function 。</p>
<p>平台层 platform<br>提供操作系统/平台相关的底层功能。</p>
<p>目前包括：</p>
<p>文件系统 file_service<br>路径 path<br>核心层 core<br>提供软件系统常用模块。</p>
<p>目前包括：</p>
<p>基础库 base （宏、哈希）<br>色彩 color<br>数学库 math<br>元数据系统 meta<br>反射 reflection<br>序列化/反序列化 serializer<br>日志系统 log<br>资源层 resource<br>提供资产加载、保存功能，资产的结构化数据定义和相关路径配置等。</p>
<p>目前包括：</p>
<p>资产系统 asset_manager<br>配置系统 config_manager<br>结构化数据定义 res_type<br>全局数据 global<br>全局粒子设置 global_particle<br>全局渲染配置 global_rendering<br>通用数据 common<br>世界 world<br>关卡 level<br>对象 object<br>组件数据 components<br>动画 animation<br>相机 camera<br>粒子发射器 emitter<br>网格 mesh<br>运动 motor<br>刚体 rigid_body<br>其他数据 data<br>动画片段 animation_clip<br>动画骨骼节点 animation_skeleton_node_map<br>基本形状 basic_shape<br>动画混合状态 blend_state<br>相机配置 camera_config<br>材质 material<br>网格数据 mesh_data<br>骨骼 skeleton_data<br>骨骼掩膜 skeleton_mask<br>功能层 function<br>提供引擎功能模块。分为框架和子系统两部分。</p>
<p>框架 framework<br>运行时功能核心框架。核心框架采用世界 world -关卡 level -GO object -组件 component 的层级架构。</p>
<p>世界管理器 world_manager 负责管理世界的加载、卸载、保存，和tick下属当前关卡。 关卡 level 负责加载、卸载、保存关卡。同时关卡也管理下属GO的tick、创建和删除。 游戏对象 object 负责加载、保存GO。同时GO也管理下属组件。</p>
<p>组件全都继承自 component.h 中的 Component 类，目前组件包括：</p>
<p>动画 animation<br>相机 camera<br>网格 mesh<br>运动 motor<br>粒子 particle<br>刚体 rigidbody<br>变换 transform<br>子系统<br>function 文件夹中 framework 文件夹之外所有部分。在具体GO组件的功能之外，运行时功能层其他子系统。</p>
<p>目前包括：</p>
<p>动画 animation<br>角色 character<br>控制器 controller<br>全局上下文 global<br>输入 input<br>粒子 particle<br>物理 physics<br>渲染 render<br>UI ui</p>
]]></content>
      <categories>
        <category>图形学</category>
      </categories>
      <tags>
        <tag>引擎</tag>
      </tags>
  </entry>
</search>
