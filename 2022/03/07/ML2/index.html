
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>机器学习基础(二) - Sunrisulfr的菠萝屋</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Sunrisulfr,"> 
    <meta name="description" content="RoadMap
逻辑斯蒂回归
支持向量机
决策树
AdaBoost 算法
梯度提升决策树 GBDT


机器学习实践

Index

符号说明
信息论
逻辑斯蒂回归
逻辑斯蒂回归模型定义
逻辑斯蒂回,"> 
    <meta name="author" content="Sunrisulfr"> 
    <link rel="alternative" href="atom.xml" title="Sunrisulfr的菠萝屋" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.1.1"></head>

<body class="loading">
    <span id="config-title" style="display:none">Sunrisulfr的菠萝屋</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="/"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">机器学习基础(二)</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">机器学习基础(二)</h1>
        <div class="stuff">
            <span>三月 07, 2022</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E5%9F%BA%E7%A1%80/" rel="tag">基础</a></li></ul>


        </div>
        <div class="content markdown">
            <h2 id="RoadMap"><a href="#RoadMap" class="headerlink" title="RoadMap"></a><strong>RoadMap</strong></h2><ul>
<li><a href="#逻辑斯蒂回归">逻辑斯蒂回归</a></li>
<li><a href="#支持向量机">支持向量机</a></li>
<li><a href="#决策树">决策树</a></li>
<li><a href="#adaboost-算法">AdaBoost 算法</a><ul>
<li><a href="#梯度提升决策树-gbdt">梯度提升决策树 GBDT</a></li>
</ul>
</li>
<li><a href="#机器学习实践">机器学习实践</a></li>
</ul>
<h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a><strong>Index</strong></h2><!-- TOC -->
<ul>
<li><a href="#符号说明">符号说明</a></li>
<li><a href="#信息论">信息论</a></li>
<li><a href="#逻辑斯蒂回归">逻辑斯蒂回归</a><ul>
<li><a href="#逻辑斯蒂回归模型定义">逻辑斯蒂回归模型定义</a></li>
<li><a href="#逻辑斯蒂回归推导">逻辑斯蒂回归推导</a></li>
<li><a href="#多分类逻辑斯蒂回归模型-todo">多分类逻辑斯蒂回归模型 TODO</a></li>
</ul>
</li>
<li><a href="#支持向量机">支持向量机</a><ul>
<li><a href="#支持向量机简述">支持向量机简述</a><ul>
<li><a href="#什么是支持向量">什么是支持向量</a></li>
<li><a href="#支持向量机的分类">支持向量机的分类</a></li>
<li><a href="#核函数与核技巧">核函数与核技巧</a></li>
<li><a href="#最大间隔超平面背后的原理">最大间隔超平面背后的原理</a></li>
</ul>
</li>
<li><a href="#支持向量机推导">支持向量机推导</a><ul>
<li><a href="#线性可分支持向量机推导">线性可分支持向量机推导</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#决策树">决策树</a><ul>
<li><a href="#信息增益与信息增益比-todo">信息增益与信息增益比 TODO</a></li>
<li><a href="#分类树---id3-决策树与-c45-决策树-todo">分类树 - ID3 决策树与 C4.5 决策树 TODO</a></li>
<li><a href="#决策树如何避免过拟合-todo">决策树如何避免过拟合 TODO</a></li>
<li><a href="#回归树---cart-决策树">回归树 - CART 决策树</a><ul>
<li><a href="#cart-回归树算法推导">CART 回归树算法推导</a></li>
<li><a href="#示例-选择切分变量与切分点">示例: 选择切分变量与切分点</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#集成学习">集成学习</a><ul>
<li><a href="#集成学习的基本策略3">集成学习的基本策略(3)</a><ul>
<li><a href="#1-boosting">1. Boosting</a><ul>
<li><a href="#boosting-策略要解决的两个基本问题">Boosting 策略要解决的两个基本问题</a></li>
</ul>
</li>
<li><a href="#2-bagging">2. Bagging</a></li>
<li><a href="#3-stacking">3. Stacking</a></li>
</ul>
</li>
<li><a href="#adaboost-算法">AdaBoost 算法</a><ul>
<li><a href="#adaboost-算法描述">AdaBoost 算法描述</a></li>
<li><a href="#adaboost-算法要点说明">AdaBoost 算法要点说明</a></li>
</ul>
</li>
<li><a href="#前向分步算法">前向分步算法</a><ul>
<li><a href="#加法模型">加法模型</a></li>
<li><a href="#前向分步算法描述">前向分步算法描述</a></li>
<li><a href="#前向分步算法与-adaboost">前向分步算法与 AdaBoost</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#梯度提升决策树-gbdt">梯度提升决策树 GBDT</a><ul>
<li><a href="#提升树-boosting-tree">提升树 Boosting Tree</a><ul>
<li><a href="#提升树算法描述">提升树算法描述</a></li>
</ul>
</li>
<li><a href="#梯度提升gb算法">梯度提升(GB)算法</a></li>
<li><a href="#gbdt-算法描述">GBDT 算法描述</a></li>
<li><a href="#xgboost-算法">XGBoost 算法</a><ul>
<li><a href="#xgboost-与-gb-的主要区别">XGBoost 与 GB 的主要区别</a></li>
<li><a href="#xgboost-的一些内部优化">XGBoost 的一些内部优化</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#随机森林">随机森林</a></li>
<li><a href="#机器学习实践">机器学习实践</a><ul>
<li><a href="#boxmuller-变换">Box–Muller 变换</a></li>
</ul>
</li>
<li><a href="#降维">降维</a><ul>
<li><a href="#svd">SVD</a></li>
<li><a href="#pca">PCA</a></li>
<li><a href="#t-sne">t-SNE</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<!-- # 什么是推导
- 给出一个问题或模型的定义，然后求其最优解的过程 -->
<h1 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h1><ul>
<li>基本遵从《统计学习方法》一书中的符号表示。</li>
<li><p>除特别说明，默认$w$为行向量，$x$为列向量，以避免在$wx$中使用转置符号；但有些公式为了更清晰区分向量与标量，依然会使用$^T$的上标，注意区分。</p>
<p>  输入实例<code>x</code>的特征向量记为：</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T"><img src="/2022/03/07/ML2/公式_20180713114026.png" height></a></div>

<p>  注意：$x_i$ 和 $x^{(i)}$ 含义不同，前者表示训练集中第 i 个实例，后者表示特征向量中的第 i 个分量；因此，通常记训练集为：</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=T=\left&space;\{&space;(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)&space;\right&space;\}"><img src="/2022/03/07/ML2/公式_20180713132400.png" height></a></div>

<blockquote>
<p>特征向量用小$n$表示维数，训练集用大$N$表示个数</p>
</blockquote>
</li>
<li><p><strong>公式说明</strong></p>
<p>  所有公式都可以<strong>点击</strong>跳转至编辑页面，但是部分公式符号会与超链接中的转义冲突；如果编辑页面的公式与本页面中的不同，可以打开源文件，通过原链接打开。</p>
</li>
</ul>
<h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><blockquote>
<p>《深度学习》 3.13 信息论</p>
<ul>
<li>信息论的基本想法是：一件不太可能的事发生，要比一件非常可能的事发生，提供更多的信息。</li>
<li>该想法可描述为以下性质：<ol>
<li>非常可能发生的事件信息量要比较少，并且极端情况下，一定能够发生的事件应该没有信息量。</li>
<li>比较不可能发生的事件具有更大的信息量。</li>
<li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。</li>
</ol>
</li>
</ul>
</blockquote>
<h3>信息熵 与 自信息</h3>

<ul>
<li><p><strong>自信息</strong>（self-information）是一种量化以上性质的函数，定义一个事件<code>x</code>的自信息为：</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=I(x)=-\log&space;P(x)"><img src="/2022/03/07/ML2/公式_20180610215339.png" height></a></div>

<blockquote>
<p>当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons）</p>
</blockquote>
</li>
<li><p>自信息只处理单个的输出。</p>
</li>
<li><p><strong>信息熵</strong>（Information-entropy）用于对整个概率分布中的<strong>不确定性总量</strong>进行量化：</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=H(\mathrm{X})=\mathbb{E}_{\mathrm{X}&space;\sim&space;P}[I(x)]=-\sum_{x&space;\in&space;\mathrm{X}}P(x)\log&space;P(x)"><img src="/2022/03/07/ML2/公式_20180610215417.png" height></a></div>

<blockquote>
<p>信息论中，记 <code>0log0 = 0</code></p>
</blockquote>
</li>
</ul>
<h3>交叉熵 与 相对熵/KL散度</h3>

<ul>
<li><p>定义 <strong>P 对 Q</strong> 的 <strong>KL 散度</strong>（Kullback-Leibler divergence）：</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=D_P(Q)=\mathbb{E}_{\mathrm{X}\sim&space;P}\left&space;[&space;\log&space;\frac{P(x)}{Q(x)}&space;\right&space;]=\sum_{x&space;\in&space;\mathrm{X}}P(x)\left&space;[&space;\log&space;P(x)-\log&space;Q(x)&space;\right&space;]"><img src="/2022/03/07/ML2/公式_20180610215445.png" height></a></div>

</li>
</ul>
<p><strong>KL 散度在信息论中度量的是哪个直观量？</strong></p>
<ul>
<li>在离散型变量的情况下， KL 散度衡量的是：当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。</li>
</ul>
<p><strong>KL散度的性质</strong>：</p>
<ul>
<li>非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的</li>
<li>不对称；D_p(q) != D_q(p)</li>
</ul>
<p><strong>交叉熵</strong>（cross-entropy）：</p>
<div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=-\mathbb{E}_{\mathrm{X}\sim&space;P}\log&space;Q(x)=-\sum_{x&space;\in&space;\mathrm{X}}P(x)\log&space;Q(x)"><img src="/2022/03/07/ML2/公式_20180610215522.png" height></a></div>

<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/haolexiao/article/details/70142571">信息量，信息熵，交叉熵，KL散度和互信息（信息增益）</a> - CSDN博客</p>
</blockquote>
<p><strong>交叉熵 与 KL 散度的关系</strong></p>
<ul>
<li><p><strong>针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度</strong>，因为 Q 并不参与被省略的那一项。</p>
  <div align="center"><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=H(P)&plus;D_P(Q)"><img src="/2022/03/07/ML2/公式_20180610215554.png" height></a></div>
</li>
<li><p>最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。</p>
<blockquote>
<p>《深度学习》 ch5.5 - 最大似然估计</p>
</blockquote>
</li>
</ul>
<h1 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h1><h2 id="逻辑斯蒂回归模型定义"><a href="#逻辑斯蒂回归模型定义" class="headerlink" title="逻辑斯蒂回归模型定义"></a>逻辑斯蒂回归模型定义</h2><!-- TODO: 符号修改，匹配神经网络中的符号表示 -->
<ul>
<li><p><strong>二项</strong>逻辑斯蒂回归模型即如下的<strong>条件概率分布</strong></p>
<!-- [![](ML2/公式_20180709113033.png)](http://www.codecogs.com/eqnedit.php?latex=P(Y=1|x)=\frac{\exp(z)}{1&plus;\exp(z)}=\frac{1}{1&plus;\exp(-z)}) -->
<p><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=P(Y=1|x"><img src="/2022/03/07/ML2/公式_20180709152707.png" alt></a>=\frac{\exp(wx)}{1&plus;\exp(wx)}=\frac{1}{1&plus;\exp(-wx)})</p>
<p><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=P(Y=0|x"><img src="/2022/03/07/ML2/公式_20180709113237.png" alt></a>=1-P(Y=1|x))</p>
<blockquote>
<p>简洁起见，省略了偏置 $b$；也可以看做将偏置扩充到了权重中</p>
</blockquote>
<p><strong>其中</strong> </p>
<!-- [![](ML2/公式_20180709113352.png)](http://www.codecogs.com/eqnedit.php?latex=P(Y=0|x)=1-P(Y=1|x)) -->
<p><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=x\in&space;\mathbf{R}^n,Y\in&space;\left&space;\{&space;0,1&space;\right&space;\}"><img src="/2022/03/07/ML2/公式_20180709113801.png" alt></a></p>
</li>
<li><p>通常会将以上两个分布记作：</p>
<p><a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&amp;P(Y=1|x"><img src="/2022/03/07/ML2/公式_20180709153307.png" alt></a>={\color{Blue}&space;\sigma(x)}\&space;&amp;P(Y=0|x)={\color{Blue}&space;1-\sigma(x)}&space;\end{aligned})</p>
</li>
</ul>
<blockquote>
<p>《统计学习方法》 6.1 逻辑斯蒂回归模型</p>
<blockquote>
<p>原书中记作 $π(x)$ 和 $1-π(x)$，这里为了跟神经网络中统一，使用 $σ$</p>
</blockquote>
</blockquote>
<h2 id="逻辑斯蒂回归推导"><a href="#逻辑斯蒂回归推导" class="headerlink" title="逻辑斯蒂回归推导"></a>逻辑斯蒂回归推导</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/daguankele/p/6549891.html">逻辑回归推导</a> - 罐装可乐 - 博客园 </p>
<ul>
<li>推导的关键点 (3)<ol>
<li>逻辑斯蒂回归的定义</li>
<li>损失函数（极大似然）</li>
<li>参数优化（梯度下降）</li>
</ol>
</li>
</ul>
</blockquote>
<ul>
<li>给定训练集 $T={(x1,y1),..,(xN,yN)}$，其中 $x ∈ R^n, y ∈ {0, 1}$</li>
</ul>
<ol>
<li><p><strong>逻辑斯蒂回归</strong>的定义：</p>
<p> <a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&amp;P(Y=1|x"><img src="/2022/03/07/ML2/公式_20180709161030.png" alt></a>={\color{Blue}&space;\sigma(x)}\&space;&amp;P(Y=0|x)={\color{Blue}&space;1-\sigma(x)}&space;\end{aligned})</p>
</li>
<li><p><strong>负对数函数</strong>作为损失函数：</p>
<p> [<img src="/2022/03/07/ML2/公式_20180713114855.png" alt>]</p>
<p> 进一步代入 $σ(x)$ 有：</p>
<p> <a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w"><img src="/2022/03/07/ML2/公式_20180713131851.png" alt></a>&amp;=-\sum_{i=1}^N&space;\left&space;[&space;{\color{Blue}&space;y_i}(w{\color{Red}&space;x_i})-\log(1&plus;\exp(w{\color{Red}&space;x_i}))&space;\right&space;]&space;\end{aligned})</p>
</li>
<li><p><strong>求梯度</strong></p>
<p> <a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;L(w"><img src="/2022/03/07/ML2/公式_20180713132107.png" alt></a>}{\partial&space;w}&amp;=-\sum<em>{i=1}^N&space;\left&space;[&space;y_ix_i-\frac{\exp(wx_i)}{1&plus;\exp(wx_i)}x_i&space;\right&space;]\&space;&amp;=\sum</em>{i=1}^N&space;[\sigma&space;(x_i)-y_i]x_i&space;\end{aligned})</p>
</li>
<li>使用<strong>梯度下降法</strong>求解参数<blockquote>
<p>深度学习/<a href="../深度学习/README.md#梯度下降法">梯度下降法</a></p>
</blockquote>
</li>
</ol>
<h2 id="多分类逻辑斯蒂回归模型-TODO"><a href="#多分类逻辑斯蒂回归模型-TODO" class="headerlink" title="多分类逻辑斯蒂回归模型 TODO"></a>多分类逻辑斯蒂回归模型 TODO</h2><ul>
<li><p>设 $Y ∈ {1,2,..K}$，则多项式逻辑斯蒂回归模型为：</p>
<p>  <a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;P(Y=k|x"><img src="/2022/03/07/ML2/公式_20180709162840.png" alt></a>&amp;=\frac{\exp(w<em>kx)}{1&plus;\sum</em>{k=1}^{K-1}&space;\exp(w<em>kx)}&space;\quad&space;k=1,2,..,K-1&space;\&space;P(Y=K|x)&amp;=\frac{1}{1&plus;\sum</em>{k=1}^{K-1}\exp(w_kx)}&space;\end{aligned})</p>
</li>
<li>类似 $Softmax$</li>
</ul>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='true'
        data-ci='f16f580b288c93fceb5d'
        data-cs='b844e802d1894562ccabe326fcf989821a9a0915'
        data-r='blogissue'
        data-o='sunrisulfr'
        data-a='sunrisulfr'
        data-d='true'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
